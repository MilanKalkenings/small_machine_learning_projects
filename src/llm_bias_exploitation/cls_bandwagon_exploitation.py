import os  # file handling
from typing import List  # type hints for improved readability
import time  # to wait some time between api calls
import numpy as np  # array handling
import pandas as pd  # data loading
from sklearn.model_selection import train_test_split  # to evaluate on data on which classifier has not been trained
import torch  # embedding handling
from transformers import AutoModel, AutoTokenizer  # embedding handling
from hugchat import hugchat  # chat with hugging face LLMs
from hugchat.login import Login  # hugging face auth
from sklearn.metrics import accuracy_score  # classification evaluation
from xgboost import XGBClassifier  # classifier


class HuggingChatHandler:
    def __init__(self):
        sign = Login("YOUR_HUGGINGFACE_MAIL_HERE", "YOUR_HUGGINGFACE_PW_HERE")
        cookies = sign.login()
        cookie_path_dir = "../../monitoring/cookies_snapshot"
        sign.saveCookiesToDir(cookie_path_dir)
        self.llm = hugchat.ChatBot(cookies=cookies.get_dict())

    def respond(self, tweet: str, label: int, guidance: int = None) -> str:
        """
        queries LLM to predict the label for a given <tweet>.
        might use a fictional majority vote <guidance>.
        does exception handling to handle api-rate limits by waiting 10 minutes.

        :param tweet: a tweet, either hateful or not
        :param label: (slightly noisy) ground truth label
        :param guidance: guidance to exploit the "LLM bandwagon effect" (if a fictional/real majority vote is presented, the LLM is biased to follow that vote)
        """
        prompt = f"""
        You are an experienced content moderator. Given is a social media post:
        '''{tweet}'''

        Your Task is to help out a friend with determining if this post should be labeled as 'hate speech' or not.
        First take a deep breath before you start working and take notes.
        Then read the post carefully, and keep it open on your desk to refer to it during the process.
        Concisely write down your chain of thoughts during the evaluation process 
        to help your friend understanding your decision making process.
        Eventually, sum up your evaluation process by ending your response 
        with a single binary integer label wrapped in the XML-Tags <final> </final>
        (0 to label the post as 'not hate speech', 1 to label the post as 'hate speech')!
        Your response MUST contain such XML-Tags!
        Your friend will get into serious trouble, if you don't work carefully!
        """
        if guidance is not None:
            prompt += f"The majority of other evaluators gave the label '{guidance}'."
        while True:
            try:
                response = str(self.llm.query(prompt)) + f"<label>{label}</label>" + f"<tweet>{tweet}</tweet>"
                break
            except Exception:
                time.sleep(600)
        return response


class Parser:
    """
    used to parse binary prediction from LLM-output string
    """
    @staticmethod
    def extract_substring(text: str, start_sub: str, end_sub: str) -> str:
        start_idx = text.find(start_sub)
        if start_idx == -1:
            return ""  # Start substring not found
        start_idx += len(start_sub)
        end_idx = text.find(end_sub, start_idx)
        if end_idx == -1:
            return ""  # End substring not found
        return text[start_idx:end_idx]

    def extract_prediction(self, text: str) -> int:
        return int(self.extract_substring(text=text, start_sub="<final>", end_sub="</final>"))

class Embedder:
    def __init__(self, checkpoint: str):
        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
        self.model = AutoModel.from_pretrained(checkpoint)

    @staticmethod
    def mean_pooling(model_output, attention_mask):
        with torch.no_grad():
            token_embeddings = model_output[0]
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def embed(self, sentence: str):
        with torch.no_grad():
            encoded_input = self.tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')
            model_output = self.model(**encoded_input)
            sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
            return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)

    def topk(self, query_tensor: torch.Tensor, tensors: torch.Tensor, k: int) -> List[int]:
        distances = [torch.norm(query_tensor - tensor, dim=0) for tensor in tensors]
        return torch.argsort(torch.tensor(distances), descending=False)[:k]


# parameters
path_monitoring = "../../monitoring/cls_bandwagon_exploitation/"
path_no_guidance = path_monitoring + "no_guidance/"
path_noisy_ground_truth_guidance = path_monitoring + "noisy_ground_truth_guidance/"
path_cls_guidance = path_monitoring + "cls_guidance/"
path_df_train = path_monitoring + "df_train.csv"
path_df_test = path_monitoring + "df_test.csv"
n_test = 200  # test instances / data points
n_train = 2000  # train instances / data points (only used to train xgboost baseline / guidance)
embedder = Embedder("thenlper/gte-small")
classifier = XGBClassifier()
parser = Parser()
inference = False


# data loading and defining target variable, done only once
if not os.path.exists(path_df_train):
    huggingchat_handler = HuggingChatHandler()
    np.random.seed(42)
    # define target variable and balance data accordingly
    df = pd.read_csv("C:\datasets\hate_speech.csv")[["class", "tweet"]]
    df_hate_speech = df[df["class"] == 0]
    df_hate_speech["class"] = 1
    df_hate_speech.index = range(len(df_hate_speech))
    df_non_hate_speech = df[df["class"] == 2]
    df_non_hate_speech["class"] = 0
    df_non_hate_speech.index = range(len(df_non_hate_speech))
    df_non_hate_speech = df_non_hate_speech.head(len(df_hate_speech))
    df = pd.concat([df_hate_speech, df_non_hate_speech]).sample(frac=1)
    # train-test split
    df_train, df_test = train_test_split(df)
    df_train.index = range(len(df_train))
    df_test.index = range(len(df_test))
    df_train.to_csv(path_df_train, index=False)
    df_test.to_csv(path_df_test, index=False)
df_train = pd.read_csv(path_df_train)
df_test = pd.read_csv(path_df_test)


# skip this part during evaluation
if inference:
    # xgboost on embeddings
    embeds_train = torch.cat([embedder.embed(df_train["tweet"][i]) for i in range(n_train)]).numpy()
    embeds_test = torch.cat([embedder.embed(df_test["tweet"][i]) for i in range(n_test)]).numpy()
    classifier.fit(embeds_train, df_train["class"][:n_train])
    print("xgboost accuracy:", accuracy_score(y_true=df_test["class"][:n_test].values, y_pred=classifier.predict(embeds_test)))
    """
    >>> xgboost accuracy: 0.89
    """

    # LLM
    for i, row in df_test.iterrows():
        if i == n_test:
            break
        # with classifier guidance (proposed method)
        if i == len(os.listdir(path_cls_guidance)):
            guidance = classifier.predict(embedder.embed(row["tweet"]))
            with open(f'{path_cls_guidance}{i}.txt', 'w') as file:
                file.write(huggingchat_handler.respond(tweet=row["tweet"], label=row["class"], guidance=guidance))
        # with ground truth guidance (to check if LLM merely follows majority vote)
        if i == len(os.listdir(path_noisy_ground_truth_guidance)):
            with open(f'{path_noisy_ground_truth_guidance}{i}.txt', 'w') as file:
                file.write(huggingchat_handler.respond(tweet=row["tweet"], label=row["class"], guidance=row["class"]))
        # without guidance (baseline)
        if i == len(os.listdir(path_no_guidance)):
            with open(f'{path_no_guidance}{i}.txt', 'w') as file:
                file.write(huggingchat_handler.respond(tweet=row["tweet"], label=row["class"]))


# evaluation
preds_cls_guidance = []
preds_no_guidance = []
preds_perfect_guidance = []
for i in range(n_test):
    with open(f"{path_cls_guidance}{i}.txt", "r") as file:
        text = file.read()
        preds_cls_guidance.append(parser.extract_prediction(text))
    with open(f"{path_noisy_ground_truth_guidance}{i}.txt", "r") as file:
        text = file.read()
        preds_perfect_guidance.append(parser.extract_prediction(text))
    with open(f"{path_no_guidance}{i}.txt", "r") as file:
        text = file.read()
        preds_no_guidance.append(parser.extract_prediction(text))
labels = df_test["class"].values[:n_test]
print("cls guidance:", accuracy_score(y_true=labels, y_pred=np.array(preds_cls_guidance)))
print("ground truth guidance:", accuracy_score(y_true=labels, y_pred=np.array(preds_perfect_guidance)))
print("no guidance:", accuracy_score(y_true=labels, y_pred=np.array(preds_no_guidance)))

"""
>>> cls guidance: 0.93
>>> ground truth guidance: 0.94
>>> no guidance: 0.9
Conclusions: 
- all of the methods involving the LLM perform better than xgboost classification on embeddings
- using the xgboost classifier as fictional majority vote leverages the "LLM bandwagon effect" and increases accuracy
- the LLM doesn't fully rely on the fictional majority vote (else the "ground truth guidance" would lead to 100% accuracy)  
"""


# ablation: where does the "noisy ground truth guidance"-approach disagree with the label?
for i in range(n_test):
    if preds_perfect_guidance[i] != labels[i]:
        print("ground truth label:", labels[i])
        print(f"tweet:\n{df_test['tweet'][i]}")
        print("\n\n")
"""
can't display the tweets here, but in fact it seems like the ground truth label is slightly noisy
and some instances are not labeled correctly - in some cases, I agree with the LLM and disagree with the label.
Conclusions:
- LLMs might support detecting data quality issues in natural language data
"""
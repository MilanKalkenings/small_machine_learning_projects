{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSim:\n",
    "    def __init__(self, blacklist_tokens: List[str] = None, whitelist_tokens: List[str] = None, thresh: float = None):\n",
    "        self.blacklist_tokens = blacklist_tokens\n",
    "        self.whitelist_tokens = whitelist_tokens\n",
    "        self.thresh = thresh\n",
    "    \n",
    "    def jaccard_token_similarity(self, string1: str , string2: str) -> int:\n",
    "        if self.whitelist_tokens is not None:\n",
    "            tok1 = [t for t in string1.split(\" \") if t in self.whitelist_tokens]\n",
    "            tok2 = [t for t in string2.split(\" \") if t in self.whitelist_tokens]\n",
    "        elif self.blacklist_tokens is not None:\n",
    "            tok1 = [t for t in string1.split(\" \") if t not in self.blacklist_tokens]\n",
    "            tok2 = [t for t in string2.split(\" \") if t not in self.blacklist_tokens]\n",
    "        else:\n",
    "            tok1 = string1.split(\" \")\n",
    "            tok2 = string2.split(\" \")\n",
    "\n",
    "        tok1 = list(set(tok1))\n",
    "        tok2 = list(set(tok2))\n",
    "        common = list(set(tok1).intersection(set(tok2)))\n",
    "\n",
    "        if (len(tok1) == 0) and (len(tok2) == 0):\n",
    "            return 0\n",
    "        sim = len(common) / (len(tok1) + len(tok2))\n",
    "        if self.thresh is not None:\n",
    "            if sim < self.thresh:\n",
    "                return 0. \n",
    "        return sim\n",
    "\n",
    "\n",
    "class SimNet:\n",
    "    def __init__(self, nodes: List[str], sim_func):\n",
    "        # build graph; node similarities = edge weights\n",
    "        g = nx.Graph()\n",
    "        sim_mat = np.zeros((len(nodes), len(nodes)))\n",
    "        for i in tqdm(range(len(nodes)), desc=\"edge calculation\"):\n",
    "            for j in range(i + 1, len(nodes)):\n",
    "                sim = sim_func(nodes[i], nodes[j])\n",
    "                node_i = nodes[i]\n",
    "                node_j = nodes[j]\n",
    "                sim_mat[i][j] = sim\n",
    "                sim_mat[j][i] = sim\n",
    "                if sim > 0:\n",
    "                    g.add_edge(node_i, node_j, weight=sim)\n",
    "                    g.add_edge(node_j, node_i, weight=sim)\n",
    "                else:\n",
    "                    g.add_node(node_i)\n",
    "                    g.add_node(node_j)\n",
    "        self.g = g\n",
    "        self.cliques = list(nx.find_cliques(g))\n",
    "        self.sim_mat = sim_mat\n",
    "\n",
    "    def sim_cn_jaccard(self, node_i: str, node_j: str, weighted: bool) -> float:\n",
    "        # individual neighbors\n",
    "        neighbors_i = set(self.g.neighbors(node_i))\n",
    "        neighbors_j = set(self.g.neighbors(node_j))\n",
    "        union = neighbors_i.union(neighbors_j)\n",
    "        intersection = neighbors_i.intersection(neighbors_j)\n",
    "        if weighted:\n",
    "            weights_total_i = sum([self.g.get_edge_data(node_i, n)[\"weight\"] for n in neighbors_i])\n",
    "            weights_total_j = sum([self.g.get_edge_data(node_j, n)[\"weight\"] for n in neighbors_j])\n",
    "            weights_total = weights_total_i + weights_total_j\n",
    "        else:\n",
    "            weights_total = len(union)\n",
    "        # common neighbors\n",
    "        if weighted:\n",
    "            weights_common = 0\n",
    "            for neighbor in intersection:\n",
    "                # Sum of edge weights from node_i and node_j to the common neighbor\n",
    "                weight_i = self.g[node_i][neighbor]['weight']\n",
    "                weight_j = self.g[node_j][neighbor]['weight']\n",
    "                weights_common += weight_i + weight_j\n",
    "        else:\n",
    "            weights_common = len(intersection)\n",
    "        if weights_total == 0:\n",
    "            return 0\n",
    "        return weights_common / weights_total\n",
    "    \n",
    "    def sim_clique_jaccard(self, node_i: str, node_j: str) -> float:\n",
    "        cliques_i = set([i for i, c in enumerate(self.cliques) if node_i in c])\n",
    "        cliques_j = set([i for i, c in enumerate(self.cliques) if node_j in c])\n",
    "        union = cliques_i.union(cliques_j)\n",
    "        intersection = cliques_i.intersection(cliques_j)\n",
    "        if len(union) == 0:\n",
    "            return 0\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def sim_preferential_attachment(self, node_i: str, node_j: str) -> float:\n",
    "        return self.g.degree(node_i) * self.g.degree(node_j)\n",
    "    \n",
    "    def shortest_path(self, node_i: str, node_j: str) -> int:\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path(self.g, source=node_i, target=node_j)\n",
    "            return len(shortest_path)\n",
    "        except nx.NetworkXNoPath:\n",
    "            return None\n",
    "        \n",
    "    def sim(self, node_i: str, node_j: str) -> float:\n",
    "        try:\n",
    "            return self.g[node_i][node_j]['weight']\n",
    "        except: \n",
    "            return 0\n",
    "        \n",
    "\n",
    "class SimNetClassifer:\n",
    "    \"\"\"\n",
    "    pd DataFrame with node_name, label, block_id\n",
    "    \"\"\"\n",
    "    def __init__(self, sim_func = None, sim_thresh: float = 0.3):\n",
    "        if sim_func == None:\n",
    "            sim_func = TextSim(thresh=sim_thresh, blacklist_tokens=[\"\", \" \"]).jaccard_token_similarity\n",
    "        self.sim_func = sim_func\n",
    "        self.cls = None\n",
    "\n",
    "    def fit(self, df_train_eval: pd.DataFrame, cls):\n",
    "        sim_net = SimNet(nodes=df_train_eval[\"node_name\"], sim_func=self.sim_func)\n",
    "        data_train_val = self._build_data(sim_net=sim_net, df=df_train_eval)\n",
    "        x = data_train_val[[\"sim_clique\", \"sim_cn\", \"pa\", \"shortest_path\", \"sim\"]]\n",
    "        y = data_train_val[\"label\"]\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "        cls.fit(X=x_train, y=y_train.values.astype(float))\n",
    "        pred = cls.predict(x_val)\n",
    "        print(\"val acc\", accuracy_score(y_pred=pred, y_true=y_val.values.astype(float)))\n",
    "        print(\"val precision\", precision_score(y_pred=pred, y_true=y_val.values.astype(float)))\n",
    "        print(\"val recall\", recall_score(y_pred=pred, y_true=y_val.values.astype(float)))\n",
    "        self.cls = cls\n",
    "    \n",
    "    def infer(self, df_train_val: pd.DataFrame, df_infer: pd.DataFrame):\n",
    "        df_all = pd.concat([df_train_val, df_infer], ignore_index=True)\n",
    "        sim_net = SimNet(nodes=df_all[\"node_name\"], sim_func=self.sim_func)\n",
    "        data_infer = self._build_data(sim_net=sim_net, df_from=df_infer, df_to=df_all, with_label=False)\n",
    "        x = data_infer[[\"sim_clique\", \"sim_cn\", \"pa\", \"shortest_path\", \"sim\"]]\n",
    "        data_infer[\"pred\"] = self.cls.predict(x)\n",
    "        pred_pos = data_infer[data_infer[\"pred\"] == 1]\n",
    "        mapping_inference = {}\n",
    "        for i, row in df_train_val.iterrows():\n",
    "            mapping_inference[row[\"node_name\"]] = row[\"label\"]\n",
    "        for inference_node in df_infer[\"node_name\"]:\n",
    "            pred_pos_node = pred_pos[pred_pos[\"nodes\"].apply(lambda tup: inference_node in tup)]\n",
    "            if len(pred_pos_node) == 0:\n",
    "                mapping_inference[inference_node] = -1\n",
    "            else:\n",
    "                # predict most common label, or form new category\n",
    "                labels = []\n",
    "                for t in pred_pos_node[\"nodes\"]:\n",
    "                    if t[0] == inference_node:\n",
    "                        other_node = t[1]\n",
    "                    else: \n",
    "                        other_node = t[0]\n",
    "                    if other_node in mapping_inference.keys():\n",
    "                        labels.append(mapping_inference[other_node])\n",
    "                    else:  # new category\n",
    "                        labels.append(max(mapping_inference.values()) + 1)\n",
    "                mapping_inference[inference_node] = max(set(labels), key=labels.count)\n",
    "        mapping_inference = pd.DataFrame({\"node_name\": mapping_inference.keys(), \"label\": mapping_inference.values()})\n",
    "        return data_infer, sim_net, mapping_inference\n",
    "\n",
    "        \n",
    "\n",
    "    def _build_data(self, sim_net, df: pd.DataFrame = None, df_from: pd.DataFrame = None, df_to: pd.DataFrame = None, with_label: bool = True):\n",
    "        if df is not None:\n",
    "            df_from = df\n",
    "            df_to = df\n",
    "        data = []\n",
    "        gb_from = df_from.groupby(\"block_id\")\n",
    "        gb_to = df_to.groupby(\"block_id\")\n",
    "        for block_id in tqdm(range(len(df_from[\"block_id\"].unique())), desc=\"building dataset\"):\n",
    "            block = df_from[\"block_id\"].unique()[block_id]\n",
    "            nodes_from = gb_from.get_group(block)[\"node_name\"].values\n",
    "            nodes_to = gb_to.get_group(block)[\"node_name\"].values\n",
    "            combinations = list(product(nodes_from, nodes_to))\n",
    "            combinations = [c for c in combinations if c[0] != c[1]]\n",
    "            combinations_temp = []\n",
    "            for c in combinations:\n",
    "                c = tuple(sorted(c))\n",
    "                if c not in combinations_temp:\n",
    "                    combinations_temp.append(c)\n",
    "            combinations = combinations_temp\n",
    "            if with_label:\n",
    "                labels_from = gb_from.get_group(block)[\"label\"].values\n",
    "                labels_to = gb_from.get_group(block)[\"label\"].values\n",
    "                labels = [l for l in labels_from] + [l for l in labels_to]\n",
    "                nodes = [n for n in nodes_from] + [n for n in nodes_to]\n",
    "                labels_map = {}\n",
    "                for i, node in enumerate(nodes):\n",
    "                    if node not in labels_map.keys():\n",
    "                        labels_map[node] = labels[i]\n",
    "                combinations_labels = [1 if labels_map[c[0]] == labels_map[c[1]] else 0 for c in combinations]\n",
    "            for i, c in enumerate(combinations):\n",
    "                data_inner = []\n",
    "                data_inner.append(sim_net.sim_clique_jaccard(c[0], c[1]))\n",
    "                data_inner.append(sim_net.sim_cn_jaccard(c[0], c[1], weighted=False))\n",
    "                data_inner.append(sim_net.sim_preferential_attachment(c[0], c[1]))\n",
    "                data_inner.append(sim_net.shortest_path(c[0], c[1]))\n",
    "                data_inner.append(sim_net.sim(c[0], c[1]))\n",
    "                data_inner.append((c[0], c[1]))\n",
    "                if with_label:\n",
    "                    data_inner.append(combinations_labels[i])\n",
    "                data.append(data_inner)\n",
    "        # fillna & scale features\n",
    "        cols = [\"sim_clique\", \"sim_cn\", \"pa\", \"shortest_path\", \"sim\", \"nodes\"]\n",
    "        if with_label:\n",
    "            cols.append(\"label\")\n",
    "        df_data = pd.DataFrame(data, columns=cols)\n",
    "        df_data[\"shortest_path\"].fillna(df_data[\"shortest_path\"].max() + 1, inplace=True)\n",
    "        scaler = MinMaxScaler()\n",
    "        for col in [\"pa\", \"shortest_path\"]:\n",
    "            df_data[col] = scaler.fit_transform(df_data[col].values.reshape(-1, 1))\n",
    "        return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "from io import BytesIO  \n",
    "\n",
    "df = pd.read_csv(BytesIO(res.content))\n",
    "df.to_csv(\"musicbrainz.csv\", index=False)\n",
    "url = \"https://raw.githubusercontent.com/tomonori-masui/entity-resolution/main/data/musicbrainz_200k.csv\"\n",
    "res = requests.get(url)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_year(year):\n",
    "    year = str(year)\n",
    "    if year in [\"n.a.\", \"unk.\"]:\n",
    "        return \"unknown\"\n",
    "    for j in range(50, 100):\n",
    "        if (year == f\"'{j}\") or (year == f\"{j}\"):\n",
    "            return f\"19{j}\"\n",
    "    for i in range(25):\n",
    "        if i < 10:\n",
    "            if year == str(i):\n",
    "                return f\"200{i}\"\n",
    "            i = f\"0{i}\"\n",
    "        if (year == f\"'{i}\") or (year == f\"{i}\") or (year == f\"{i}\"):\n",
    "            return f\"20{i}\"\n",
    "    return year\n",
    "\n",
    "\n",
    "# load and clean data, split into data available at training time and data to infer labels on\n",
    "df = pd.read_csv(\"musicbrainz.csv\")\n",
    "df = df[df[\"language\"].isin([\"German\", \"ger.\", \"Ger.\", \"german\", \"GERMAN\"])].drop_duplicates(\"title\").sort_values(\"CID\")\n",
    "df = df[df[\"year\"].notna()]\n",
    "df[\"year\"] = df[\"year\"].apply(clean_year)\n",
    "df[\"year\"].value_counts().head(50)\n",
    "df = df[df[\"title\"].notna()]\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.replace(\"-\", \" \"))\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.replace(\"'\", \" \"))\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.replace('\"', \" \"))\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: re.sub(pattern=\"\\d\\d\\d\", repl=\"\", string=x))\n",
    "df = df.groupby(\"year\").filter(lambda x: (len(x) > 1) and (len(x) < 75))\n",
    "#df = df.groupby(\"CID\").filter(lambda x: (len(x) > 1))\n",
    "df = df[[\"CID\", \"title\", \"year\"]]\n",
    "df[\"CID\"] = LabelEncoder().fit_transform(df[\"CID\"])\n",
    "df.columns = [\"label\", \"node_name\", \"block_id\"]\n",
    "df.index = range(len(df))\n",
    "df_train_eval, df_inference = train_test_split(df, test_size=0.2)\n",
    "df_train_eval.index = range(len(df_train_eval))\n",
    "df_inference.index = range(len(df_inference))\n",
    "df_inference[\"label\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edge calculation:   0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edge calculation: 100%|██████████| 941/941 [00:04<00:00, 211.01it/s]\n",
      "building dataset: 100%|██████████| 44/44 [00:02<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.9976195628651807\n",
      "val precision 0.9347826086956522\n",
      "val recall 0.8431372549019608\n"
     ]
    }
   ],
   "source": [
    "smc = SimNetClassifer(sim_thresh=0.2)\n",
    "cls = RandomForestClassifier(n_estimators=1000, min_samples_leaf=2)\n",
    "smc.fit(df_train_eval=df_train_eval, cls=cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edge calculation: 100%|██████████| 1177/1177 [00:07<00:00, 167.16it/s]\n",
      "building dataset: 100%|██████████| 37/37 [00:01<00:00, 29.15it/s]\n"
     ]
    }
   ],
   "source": [
    "preds, sim_net, mapping_inference = smc.infer(df_train_val=df_train_eval, df_infer=df_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x22bdf8ccb20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize network\n",
    "from pyvis.network import Network\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "norm = mcolors.Normalize(\n",
    "    vmin=min(mapping_inference[\"label\"].values), \n",
    "    vmax=max(mapping_inference[\"label\"].values), \n",
    "    clip=True)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=cm.cividids)\n",
    "\n",
    "net = Network(notebook=True, cdn_resources='remote')\n",
    "\n",
    "# colored vertices\n",
    "for node in sim_net.g.nodes:\n",
    "    color = mcolors.to_hex(mapper.to_rgba(mapping_inference[mapping_inference[\"node_name\"] == str(node)][\"label\"].values[0]))\n",
    "    net.add_node(n_id=node, label=str(node), color=color, value=1)\n",
    "# edges\n",
    "for edge in sim_net.g.edges:\n",
    "    net.add_edge(edge[0], edge[1])\n",
    "\n",
    "net.show('graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"names_german.txt\", \"r\", encoding=\"utf8\") as f:\\n    names = f.read().split(\"\\n\")\\nnames_temp = []\\nfor name in names:\\n    split = name.split(\" \")\\n    if len(split) == 2:\\n        names_temp.extend(split)\\nnames = list(set(names_temp))\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvec = TfidfVectorizer(vocabulary=names, use_idf=False, lowercase=False).fit(df[\"title\"])\\ntfidf = pd.DataFrame(vec.transform(df[\"title\"]).todense() > 0, columns=vec.get_feature_names_out())\\ntfidf = tfidf[tfidf.columns[tfidf.sum() > 1]]\\n\\nnames_contained = []\\nfor i, row in tfidf.iterrows():\\n    names_contained.append([c for c in tfidf.columns[row.values]])\\nnames_contained\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blocking by \"contains same name\"\n",
    "\"\"\"\n",
    "with open(\"names_german.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "names_temp = []\n",
    "for name in names:\n",
    "    split = name.split(\" \")\n",
    "    if len(split) == 2:\n",
    "        names_temp.extend(split)\n",
    "names = list(set(names_temp))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(vocabulary=names, use_idf=False, lowercase=False).fit(df[\"title\"])\n",
    "tfidf = pd.DataFrame(vec.transform(df[\"title\"]).todense() > 0, columns=vec.get_feature_names_out())\n",
    "tfidf = tfidf[tfidf.columns[tfidf.sum() > 1]]\n",
    "\n",
    "names_contained = []\n",
    "for i, row in tfidf.iterrows():\n",
    "    names_contained.append([c for c in tfidf.columns[row.values]])\n",
    "names_contained\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

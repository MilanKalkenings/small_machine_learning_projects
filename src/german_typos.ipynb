{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import epitran  # phonetics\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. create sentences with common typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StringManipulator:\n",
    "    @staticmethod\n",
    "    def manipulate_words(string: str, func, min_p: float = 0.1, max_p: float = 0.5) -> str:\n",
    "        \"\"\"\n",
    "        manipulate each words with a probability between min_p and max_p\n",
    "        \"\"\"\n",
    "        words = string.split(\" \")\n",
    "        # determine which words to manipulate\n",
    "        p = random.uniform(min_p, max_p)\n",
    "        pos_manipulate = [i for i in range(len(words)) if random.random() <= p]\n",
    "        # manipulate\n",
    "        string_final = \"\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word != \"\":\n",
    "                if i in pos_manipulate:\n",
    "                    string_final += \" \" + random.choice(func(word))\n",
    "                else:\n",
    "                    string_final += \" \" + word\n",
    "        return string_final[1:]\n",
    "\n",
    "\n",
    "class Typo(StringManipulator):\n",
    "    \"\"\"\n",
    "    uses german keyboard layout by default (only letters)\n",
    "    \"\"\"\n",
    "    def __init__(self, alternative_layout: dict = None) -> None:\n",
    "        super().__init__()\n",
    "        self.epi = epitran.Epitran('deu-Latn')\n",
    "        if alternative_layout is not None:\n",
    "            layout = alternative_layout\n",
    "        else:\n",
    "            layout = {\n",
    "            'q': (0, 0), 'w': (0, 1), 'e': (0, 2), 'r': (0, 3), 't': (0, 4), 'z': (0, 5), 'u': (0, 6), 'i': (0, 7), 'o': (0, 8), 'p': (0, 9), 'ü': (0, 10),\n",
    "            'a': (1, 0), 's': (1, 1), 'd': (1, 2), 'f': (1, 3), 'g': (1, 4), 'h': (1, 5), 'j': (1, 6), 'k': (1, 7), 'l': (1, 8), 'ö': (1, 9), 'ä': (1, 10),\n",
    "            'y': (2, 0), 'x': (2, 1), 'c': (2, 2), 'v': (2, 3), 'b': (2, 4), 'n': (2, 5), 'm': (2, 6)}\n",
    "        # euclidian distance\n",
    "        coords = np.array(list(layout.values()))\n",
    "        dist_matrix = np.sqrt(np.sum((coords[:, np.newaxis, :] - coords[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "        for i in range(len(dist_matrix)):\n",
    "            dist_matrix[i][i] = np.inf  # main diagonal = inf to ignore replacing key with itself\n",
    "        self.dist_df = pd.DataFrame(data=dist_matrix, columns=list(layout.keys()), index=list(layout.keys()))\n",
    "\n",
    "    @staticmethod\n",
    "    def lastvowels_positions(string: str) -> List[int]:\n",
    "        return [v.end() -1 for v in re.finditer(pattern=\"[aeiouäöüAEIOUÄÖÜ]+\", string=string)]\n",
    "    \n",
    "    def german_phonetics(self, string: str) -> str:\n",
    "        return self.epi.transliterate(string)\n",
    "\n",
    "    def all(self, string: str, balanced: bool = True) -> List[str]:\n",
    "        d = self.deletion(string)\n",
    "        t = self.transposition(string)\n",
    "        i = self.insertion(string)\n",
    "        ki = self.keyboard_substitution(string=string)\n",
    "        p = self.phonetics(string)\n",
    "        if balanced:\n",
    "            k = max(min(len(d), len(t), len(i), len(ki), len(p)), 1)\n",
    "            if len(d) != 0:\n",
    "                d = random.choices(d, k=k)\n",
    "            if len(t) != 0:\n",
    "                t = random.choices(t, k=k)\n",
    "            if len(i) != 0:\n",
    "                i = random.choices(i, k=k)\n",
    "            if len(ki) != 0:\n",
    "                ki = random.choices(ki, k=k)\n",
    "            if len(p) != 0:\n",
    "                p = random.choices(p, k=k)\n",
    "        return list(set(d + t + i + ki + p))\n",
    "\n",
    "    def deletion(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string)):\n",
    "            variants.append(string[:pos] + string[(pos + 1):])\n",
    "        return list(set(variants))\n",
    "    \n",
    "    def transposition(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string) - 1):\n",
    "            if (pos + 2) < len(string):\n",
    "                variants.append(string[:pos] + string[pos + 1] + string[pos] + string[(pos + 2):])\n",
    "            else:\n",
    "                variants.append(string[:pos] + string[pos + 1] + string[pos])\n",
    "        if string in variants:\n",
    "            variants.remove(string)\n",
    "        return variants\n",
    "    \n",
    "    def insertion(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string)):\n",
    "            variants.append(string[:pos] + string[pos] + string[pos:])\n",
    "        return list(set(variants))\n",
    "\n",
    "    def keyboard_substitution(self, string: str, max_dist: float = 1, char_positions: List[int] = None) -> List[str]:\n",
    "        if char_positions is None:\n",
    "            char_positions = range(len(string))\n",
    "        variants = []\n",
    "        for pos in char_positions:\n",
    "            char = string[pos]\n",
    "            if char in self.dist_df.index:\n",
    "                repl_with = self.dist_df[char][self.dist_df[char] <= max_dist].index\n",
    "                variants.extend([string[:pos] + c + string[(pos + 1):] for c in repl_with])\n",
    "        return variants\n",
    "    \n",
    "    def phonetics(self, string: str, check_phonetics: bool = True) -> List[str]:\n",
    "        def insert_h(string: str, match_nrs: int = None):\n",
    "            if match_nrs is None:\n",
    "                match_nrs = range(len(string))\n",
    "            string_out = \"\"\n",
    "            match_nr = -1\n",
    "            pos_lasts = self.lastvowels_positions(string)\n",
    "            for i in range(len(string)):\n",
    "                string_out += string[i]\n",
    "                if i in pos_lasts:\n",
    "                    if i != 0:\n",
    "                        if string[i] != string[i - 1]:\n",
    "                            if len(string) >= (i + 3):\n",
    "                                if (string[i + 1] != string[i + 2]) and (string[i+1:i+2] != \"dt\"):\n",
    "                                    match_nr += 1\n",
    "                                    if match_nr in match_nrs:\n",
    "                                        string_out += \"h\"\n",
    "                            else:\n",
    "                                match_nr +=1\n",
    "                                if match_nr in match_nrs:\n",
    "                                    string_out += \"h\"\n",
    "                    else:\n",
    "                        match_nr += 1\n",
    "                        if match_nr in match_nrs:\n",
    "                            string_out += \"h\"\n",
    "            return string_out\n",
    "        variants = []\n",
    "        string_phonetics = self.german_phonetics(string)\n",
    "        # h after vowel\n",
    "        for i in range(len(self.lastvowels_positions(string))):\n",
    "            variant = insert_h(string=string, match_nrs=[i])\n",
    "            if self.german_phonetics(variant) == string_phonetics:\n",
    "                variants.append(variant)\n",
    "        # ß = ss\n",
    "        variants.append(re.sub(pattern=\"ß\", repl=\"ss\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ss\", repl=\"ß\", string=string))    \n",
    "        # dt = tt\n",
    "        variants.append(re.sub(pattern=\"tt\", repl=\"dt\", string=string))\n",
    "        variants.append(re.sub(pattern=\"dt\", repl=\"tt\", string=string))\n",
    "        # ld = lt\n",
    "        variants.append(re.sub(pattern=\"lt\", repl=\"ld\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ld\", repl=\"lt\", string=string))\n",
    "        # r = -\n",
    "        variants.append(re.sub(pattern=\"r\", repl=\"\", string=string))\n",
    "        # er = a\n",
    "        variants.append(re.sub(pattern=\"er\", repl=\"a\", string=string))\n",
    "        variants.append(re.sub(pattern=\"a\", repl=\"er\", string=string))\n",
    "        # x = ks = cks\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])c?ks\", repl=\"x\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])x\", repl=\"ks\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])x\", repl=\"cks\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])cks\", repl=\"x\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])ks\", repl=\"cks\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])cks\", repl=\"ks\", string=string))\n",
    "        # ck = k\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])k\", repl=\"ck\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])ck\", repl=\"k\", string=string))\n",
    "        # gk = k\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])k\", repl=\"gk\", string=string))\n",
    "        variants.append(re.sub(pattern=\"(?<=[aeiouäöüAEIOUÄÖÜ])gk\", repl=\"k\", string=string))\n",
    "        # z = ts\n",
    "        variants.append(re.sub(pattern=\"z\", repl=\"ts\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ts\", repl=\"z\", string=string))\n",
    "        # h = j\n",
    "        variants.append(re.sub(pattern=\"g\", repl=\"j\", string=string))\n",
    "        variants.append(re.sub(pattern=\"j\", repl=\"g\", string=string))\n",
    "        # ph = f = v\n",
    "        variants.append(re.sub(pattern=\"ph\", repl=\"f\", string=string))\n",
    "        variants.append(re.sub(pattern=\"f\", repl=\"ph\", string=string))\n",
    "        variants.append(re.sub(pattern=\"v\", repl=\"ph\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ph\", repl=\"v\", string=string))\n",
    "        variants.append(re.sub(pattern=\"f\", repl=\"v\", string=string))\n",
    "        variants.append(re.sub(pattern=\"v\", repl=\"f\", string=string))\n",
    "        # ie = i\n",
    "        variants.append(re.sub(pattern=\"ie\", repl=\"i\", string=string))\n",
    "        variants.append(re.sub(pattern=\"i\", repl=\"ie\", string=string))\n",
    "        # e = ä\n",
    "        variants.append(re.sub(pattern=\"e\", repl=\"ä\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ä\", repl=\"e\", string=string))\n",
    "\n",
    "        variants = list(set(variants))\n",
    "        variants.remove(string)\n",
    "\n",
    "        if check_phonetics:\n",
    "            for v in variants:\n",
    "                if self.german_phonetics(v) != string_phonetics:\n",
    "                    variants.remove(v)\n",
    "        return variants\n",
    "\n",
    "\n",
    "class OCR(StringManipulator):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def whitespace(string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for i in range(1, len(string)):\n",
    "            variants.append(string[:i] + \" \" + string[i:])\n",
    "        return variants\n",
    "\n",
    "    @staticmethod\n",
    "    def fragment(string: str) -> List[str]:\n",
    "        \"\"\"especially for badly scanned docs, small fragments might be misinterpreted as chars\"\"\"\n",
    "        variants = []\n",
    "        for fragment in \".,'`´\":\n",
    "            for i in range(len(string)):\n",
    "                variants.append(string[:i] + fragment + string[i:])\n",
    "        return variants\n",
    "\n",
    "    @staticmethod\n",
    "    def look(string: str) -> List[str]:\n",
    "        variants = []\n",
    "        # rn = m\n",
    "        variants.append(re.sub(pattern=\"rn\", repl=\"m\", string=string))\n",
    "        variants.append(re.sub(pattern=\"m\", repl=\"rn\", string=string))\n",
    "        # e = c\n",
    "        variants.append(re.sub(pattern=\"e\", repl=\"c\", string=string))\n",
    "        variants.append(re.sub(pattern=\"c\", repl=\"e\", string=string))\n",
    "        # O = 0 = o\n",
    "        variants.append(re.sub(pattern=\"O\", repl=\"0\", string=string))\n",
    "        variants.append(re.sub(pattern=\"O\", repl=\"o\", string=string))\n",
    "        variants.append(re.sub(pattern=\"o\", repl=\"0\", string=string))\n",
    "        variants.append(re.sub(pattern=\"o\", repl=\"O\", string=string))\n",
    "        variants.append(re.sub(pattern=\"0\", repl=\"o\", string=string))\n",
    "        variants.append(re.sub(pattern=\"0\", repl=\"O\", string=string))\n",
    "        # v = u\n",
    "        variants.append(re.sub(pattern=\"v\", repl=\"u\", string=string))\n",
    "        variants.append(re.sub(pattern=\"u\", repl=\"v\", string=string))\n",
    "        variants.append(re.sub(pattern=\"V\", repl=\"U\", string=string))\n",
    "        variants.append(re.sub(pattern=\"U\", repl=\"V\", string=string))\n",
    "        # . = ,\n",
    "        variants.append(re.sub(pattern=\"\\.\", repl=\",\", string=string))\n",
    "        variants.append(re.sub(pattern=\",\", repl=\".\", string=string))\n",
    "        # : = ;\n",
    "        variants.append(re.sub(pattern=\":\", repl=\";\", string=string))\n",
    "        variants.append(re.sub(pattern=\";\", repl=\":\", string=string))\n",
    "        # ! = I = 1\n",
    "        variants.append(re.sub(pattern=\"I\", repl=\"1\", string=string))\n",
    "        variants.append(re.sub(pattern=\"I\", repl=\"!\", string=string))\n",
    "        variants.append(re.sub(pattern=\"!\", repl=\"1\", string=string))\n",
    "        variants.append(re.sub(pattern=\"!\", repl=\"I\", string=string))\n",
    "        variants.append(re.sub(pattern=\"1\", repl=\"!\", string=string))\n",
    "        variants.append(re.sub(pattern=\"1\", repl=\"I\", string=string))\n",
    "        # o = ö\n",
    "        variants.append(re.sub(pattern=\"o\", repl=\"ö\", string=string))\n",
    "        variants.append(re.sub(pattern=\"ö\", repl=\"o\", string=string))\n",
    "        variants.append(re.sub(pattern=\"O\", repl=\"Ö\", string=string))\n",
    "        variants.append(re.sub(pattern=\"Ö\", repl=\"O\", string=string))\n",
    "        # h = n\n",
    "        variants.append(re.sub(pattern=\"h\", repl=\"n\", string=string))\n",
    "        variants.append(re.sub(pattern=\"n\", repl=\"h\", string=string))\n",
    "        # 5 = s\n",
    "        variants.append(re.sub(pattern=\"5\", repl=\"s\", string=string))\n",
    "        variants.append(re.sub(pattern=\"s\", repl=\"5\", string=string))\n",
    "        variants.append(re.sub(pattern=\"5\", repl=\"S\", string=string))\n",
    "        variants.append(re.sub(pattern=\"S\", repl=\"5\", string=string))\n",
    "        # 8 = B\n",
    "        variants.append(re.sub(pattern=\"8\", repl=\"B\", string=string))\n",
    "        variants.append(re.sub(pattern=\"B\", repl=\"8\", string=string))\n",
    "        # vv = w\n",
    "        variants.append(re.sub(pattern=\"vv\", repl=\"w\", string=string))\n",
    "        variants.append(re.sub(pattern=\"w\", repl=\"vv\", string=string))\n",
    "        variants.append(re.sub(pattern=\"VV\", repl=\"W\", string=string))\n",
    "        variants.append(re.sub(pattern=\"W\", repl=\"VV\", string=string))\n",
    "        # ij = u\n",
    "        variants.append(re.sub(pattern=\"ij\", repl=\"u\", string=string))\n",
    "        variants.append(re.sub(pattern=\"u\", repl=\"ij\", string=string))\n",
    "        # cl = d\n",
    "        variants.append(re.sub(pattern=\"cl\", repl=\"d\", string=string))\n",
    "        variants.append(re.sub(pattern=\"d\", repl=\"cl\", string=string))\n",
    "        # - = _\n",
    "        variants.append(re.sub(pattern=\"-\", repl=\"_\", string=string))\n",
    "        variants.append(re.sub(pattern=\"_\", repl=\"-\", string=string))\n",
    "        # v = V\n",
    "        variants.append(re.sub(pattern=\"v\", repl=\"V\", string=string))\n",
    "        variants.append(re.sub(pattern=\"V\", repl=\"v\", string=string))\n",
    "        # w = W\n",
    "        variants.append(re.sub(pattern=\"w\", repl=\"W\", string=string))\n",
    "        variants.append(re.sub(pattern=\"W\", repl=\"w\", string=string))\n",
    "        # s = S\n",
    "        variants.append(re.sub(pattern=\"s\", repl=\"S\", string=string))\n",
    "        variants.append(re.sub(pattern=\"S\", repl=\"s\", string=string))\n",
    "        # ´ = '\n",
    "        variants.append(re.sub(pattern=\"´\", repl=\"'\", string=string))\n",
    "        variants.append(re.sub(pattern=\"'\", repl=\"´\", string=string))\n",
    "        # ` = '\n",
    "        variants.append(re.sub(pattern=\"`\", repl=\"'\", string=string))\n",
    "        variants.append(re.sub(pattern=\"'\", repl=\"`\", string=string))\n",
    "\n",
    "        variants = list(set(variants))\n",
    "        variants.remove(string) \n",
    "        return variants\n",
    "    \n",
    "    def all(self, string: str, balanced: bool = True, p_w: float = 0.15, p_f: float = 0.15) -> List[str]:\n",
    "        p_l = 1 - (p_w + p_f) # TODO implement probability based \n",
    "        w = self.whitespace(string)\n",
    "        f = self.fragment(string)\n",
    "        l = self.look(string)\n",
    "        if balanced:\n",
    "            k = max(min(len(w), len(f), len(l)), 1)\n",
    "            w = random.choices(w, k=k)\n",
    "            f = random.choices(f, k=k)\n",
    "            l = random.choices(l, k=k)\n",
    "        return list(set(w + f + l))\n",
    "    \n",
    "\n",
    "def check_for_abbreviations(string: str):\n",
    "    matches = re.finditer(pattern=\"\\w+\\.\\s\", string=string)\n",
    "    match_strs = []\n",
    "    for match in matches:\n",
    "        match_strs.append(string[match.start(): match.end()])\n",
    "    print(pd.Series(match_strs).value_counts().head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"recipes.json\", \"rb\") as f:\n",
    "    j = json.load(f)\n",
    "instructions = \". \".join(list(set([j[i][\"Instructions\"] for i in range(len(j))])))\n",
    "\n",
    "\n",
    "# delete . from abbreviations to allow splitting per .\n",
    "instructions = re.sub(pattern=\"ca\\.\", repl=\"ca\", string=instructions)\n",
    "instructions = re.sub(pattern=\"Min\\.\", repl=\"Min\", string=instructions)\n",
    "instructions = re.sub(pattern=\"min\\.\", repl=\"min\", string=instructions)\n",
    "instructions = re.sub(pattern=\"evtl\\.\", repl=\"evtl\", string=instructions)\n",
    "instructions = re.sub(pattern=\"bzw\\.\", repl=\"bzw\", string=instructions)\n",
    "instructions = re.sub(pattern=\"Ca\\.\", repl=\"ca\", string=instructions)\n",
    "instructions = re.sub(pattern=\"B\\.\", repl=\"B\", string=instructions)\n",
    "instructions = re.sub(pattern=\"[0-9]\\.\", repl=\"\", string=instructions)\n",
    "sentences = pd.Series([s.strip() for s in instructions.split(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom german_spelling import Typo\\n\\ntypo = Typo()\\nversions = []\\nfor i in range(36):\\n    print(\"version\", i)\\n    sentences_typo = []\\n    for j, sentence in enumerate(sentences):\\n        sentences_typo.append(typo.manipulate_words(string=sentence, min_p=0.1, max_p=0.3, func=typo.all))\\n    pd.Series(sentences_typo).to_frame().to_csv(f\"recipe_variants/{i}.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used to generate faulty sentences\n",
    "\"\"\"\n",
    "typo = Typo()\n",
    "versions = []\n",
    "for i in range(36):\n",
    "    print(\"version\", i)\n",
    "    sentences_typo = []\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        sentences_typo.append(typo.manipulate_words(string=sentence, min_p=0.1, max_p=0.3, func=typo.all))\n",
    "    pd.Series(sentences_typo).to_frame().to_csv(f\"recipe_variants/{i}.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train sequence classifier to detect sentences with typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_tensor[idx], self.target_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "# load original sentences and faulty sentences and split them into train and test sets\n",
    "variants = []\n",
    "for i in range(36):\n",
    "    variants.append(pd.read_csv(f\"recipe_variants/{i}.csv\"))\n",
    "variants = pd.concat(variants, axis=1)\n",
    "\n",
    "variants.columns = [str(i) for i in range(36)]\n",
    "variants.eq(sentences, axis=0).any(axis=1).mean()\n",
    "for col in variants.columns:\n",
    "    variants[col] = variants[col].where(variants[col] != sentences, \"\")\n",
    "\n",
    "variants = variants[sentences != \"\"].dropna()\n",
    "negatives_train = variants.head(30_000)\n",
    "negatives_test = variants[~variants.index.isin(negatives_train.index)]\n",
    "negatives_train = negatives_train.values.reshape(30_000 * 36)\n",
    "negatives_train = negatives_train[negatives_train != \"\"]\n",
    "negatives_test = negatives_test.values.reshape(13746 * 36)\n",
    "negatives_test = negatives_test[negatives_test != \"\"]\n",
    "positives_train = sentences.head(30_000)\n",
    "positives_test = sentences[~sentences.index.isin(positives_train.index)]\n",
    "negatives_train = pd.Series(negatives_train).sample(frac=1).values[:len(positives_train)]\n",
    "negatives_test = pd.Series(negatives_train).sample(frac=1).values[:len(positives_test)]\n",
    "\n",
    "# tokenize sentences to form input ids (usually it is recommended to use masking)\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "negatives_train_tokens = torch.tensor(tok(list(negatives_train), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "positives_train_tokens = torch.tensor(tok(list(positives_train), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "\n",
    "negatives_test_tokens = torch.tensor(tok(list(negatives_test), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "positives_test_tokens = torch.tensor(tok(list(positives_test), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "\n",
    "inputs_train = torch.cat([negatives_train_tokens, positives_train_tokens])\n",
    "inputs_test = torch.cat([negatives_test_tokens, positives_test_tokens])\n",
    "\n",
    "# create target tensors\n",
    "train_target = torch.ones(len(negatives_train_tokens) + len(positives_train_tokens), dtype=int)\n",
    "train_target[:len(negatives_train_tokens)] = 0\n",
    "test_target = torch.ones(len(negatives_test_tokens) + len(positives_test_tokens), dtype=int)\n",
    "test_target[:len(negatives_test_tokens)] = 0\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "dataset_train = CustomDataset(inputs_train, train_target)\n",
    "dataset_test = CustomDataset(inputs_test[:1024], test_target[:1024])\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.590029776096344\n",
      "test: 0.77734375\n",
      "train: 0.8221726417541504\n",
      "test: 0.70703125\n",
      "train: 0.8288690447807312\n",
      "test: 0.748046875\n",
      "train: 0.8563988208770752\n",
      "test: 0.7734375\n",
      "train: 0.8526785969734192\n",
      "test: 0.7783203125\n",
      "train: 0.8623511791229248\n",
      "test: 0.79296875\n",
      "train: 0.8549107313156128\n",
      "test: 0.7265625\n",
      "train: 0.8571428656578064\n",
      "test: 0.736328125\n",
      "train: 0.8333333134651184\n",
      "test: 0.7529296875\n",
      "train: 0.883184552192688\n",
      "test: 0.7578125\n",
      "train: 0.8459821343421936\n",
      "test: 0.8076171875\n",
      "train: 0.8422619104385376\n",
      "test: 0.7041015625\n",
      "train: 0.8623511791229248\n",
      "test: 0.765625\n",
      "train: 0.882440447807312\n",
      "test: 0.7578125\n",
      "train: 0.8616071343421936\n",
      "test: 0.8212890625\n",
      "train: 0.8534226417541504\n",
      "test: 0.7060546875\n",
      "train: 0.866815447807312\n",
      "test: 0.7744140625\n",
      "train: 0.8623511791229248\n",
      "test: 0.763671875\n",
      "train: 0.8623511791229248\n",
      "test: 0.7353515625\n",
      "train: 0.875\n",
      "test: 0.783203125\n",
      "train: 0.8563988208770752\n",
      "test: 0.7646484375\n",
      "train: 0.8809523582458496\n",
      "test: 0.7919921875\n",
      "train: 0.8482142686843872\n",
      "test: 0.7939453125\n",
      "train: 0.8683035969734192\n",
      "test: 0.775390625\n",
      "train: 0.8742559552192688\n",
      "test: 0.7626953125\n",
      "train: 0.8757440447807312\n",
      "test: 0.80078125\n",
      "train: 0.840029776096344\n",
      "test: 0.7734375\n",
      "train: 0.882440447807312\n",
      "test: 0.8037109375\n",
      "train: 0.8742559552192688\n",
      "test: 0.779296875\n",
      "train: 0.8422619104385376\n",
      "test: 0.763671875\n",
      "train: 0.863095223903656\n",
      "test: 0.7880859375\n",
      "train: 0.8876488208770752\n",
      "test: 0.802734375\n",
      "train: 0.8816964030265808\n",
      "test: 0.779296875\n",
      "train: 0.8802083134651184\n",
      "test: 0.765625\n",
      "train: 0.8690476417541504\n",
      "test: 0.7890625\n",
      "train: 0.8764880895614624\n",
      "test: 0.767578125\n",
      "train: 0.8623511791229248\n",
      "test: 0.791015625\n",
      "train: 0.859375\n",
      "test: 0.7568359375\n",
      "train: 0.8653273582458496\n",
      "test: 0.7822265625\n",
      "train: 0.8720238208770752\n",
      "test: 0.7548828125\n",
      "train: 0.867559552192688\n",
      "test: 0.7578125\n",
      "train: 0.8757440447807312\n",
      "test: 0.794921875\n",
      "train: 0.8720238208770752\n",
      "test: 0.8115234375\n",
      "train: 0.8720238208770752\n",
      "test: 0.76171875\n",
      "train: 0.8638392686843872\n",
      "test: 0.7978515625\n",
      "train: 0.8913690447807312\n",
      "test: 0.787109375\n",
      "train: 0.8653273582458496\n",
      "test: 0.802734375\n",
      "train: 0.8727678656578064\n",
      "test: 0.771484375\n",
      "train: 0.8660714030265808\n",
      "test: 0.7919921875\n",
      "train: 0.8735119104385376\n",
      "test: 0.7587890625\n",
      "train: 0.8727678656578064\n",
      "test: 0.8076171875\n",
      "train: 0.8757440447807312\n",
      "test: 0.77734375\n",
      "train: 0.8645833134651184\n",
      "test: 0.787109375\n",
      "train: 0.8809523582458496\n",
      "test: 0.7978515625\n",
      "train: 0.8683035969734192\n",
      "test: 0.8154296875\n",
      "train: 0.8638392686843872\n",
      "test: 0.79296875\n",
      "train: 0.8601190447807312\n",
      "test: 0.80078125\n",
      "train: 0.8623511791229248\n",
      "test: 0.8076171875\n",
      "train: 0.8794642686843872\n",
      "test: 0.8037109375\n",
      "train: 0.8876488208770752\n",
      "test: 0.7900390625\n",
      "train: 0.8727678656578064\n",
      "test: 0.8095703125\n",
      "train: 0.8809523582458496\n",
      "test: 0.7900390625\n",
      "train: 0.882440447807312\n",
      "test: 0.8154296875\n",
      "train: 0.8913690447807312\n",
      "test: 0.7958984375\n",
      "train: 0.886904776096344\n",
      "test: 0.8017578125\n",
      "train: 0.8898809552192688\n",
      "test: 0.8212890625\n",
      "train: 0.902529776096344\n",
      "test: 0.810546875\n",
      "train: 0.8883928656578064\n",
      "test: 0.80078125\n",
      "train: 0.883184552192688\n",
      "test: 0.8056640625\n",
      "train: 0.890625\n",
      "test: 0.7998046875\n",
      "train: 0.886904776096344\n",
      "test: 0.826171875\n",
      "train: 0.8764880895614624\n",
      "test: 0.8095703125\n",
      "train: 0.882440447807312\n",
      "test: 0.798828125\n",
      "train: 0.8898809552192688\n",
      "test: 0.810546875\n",
      "train: 0.8936011791229248\n",
      "test: 0.796875\n",
      "train: 0.9092261791229248\n",
      "test: 0.814453125\n",
      "train: 0.890625\n",
      "test: 0.8076171875\n",
      "train: 0.8854166865348816\n",
      "test: 0.8154296875\n",
      "train: 0.902529776096344\n",
      "test: 0.8251953125\n",
      "train: 0.8965773582458496\n",
      "test: 0.8056640625\n",
      "train: 0.8772321343421936\n",
      "test: 0.8125\n",
      "train: 0.8690476417541504\n",
      "test: 0.791015625\n",
      "train: 0.8720238208770752\n",
      "test: 0.7998046875\n",
      "train: 0.886904776096344\n",
      "test: 0.787109375\n",
      "train: 0.882440447807312\n",
      "test: 0.794921875\n",
      "train: 0.8839285969734192\n",
      "test: 0.80078125\n",
      "train: 0.8854166865348816\n",
      "test: 0.841796875\n",
      "train: 0.8697916865348816\n",
      "test: 0.806640625\n",
      "train: 0.8757440447807312\n",
      "test: 0.8056640625\n",
      "train: 0.8846726417541504\n",
      "test: 0.794921875\n",
      "train: 0.8764880895614624\n",
      "test: 0.798828125\n",
      "train: 0.8846726417541504\n",
      "test: 0.794921875\n",
      "train: 0.8764880895614624\n",
      "test: 0.7939453125\n",
      "train: 0.8839285969734192\n",
      "test: 0.802734375\n",
      "train: 0.8839285969734192\n",
      "test: 0.8720703125\n",
      "train: 0.8928571343421936\n",
      "test: 0.7958984375\n",
      "train: 0.882440447807312\n",
      "test: 0.806640625\n",
      "train: 0.8965773582458496\n",
      "test: 0.7998046875\n",
      "train: 0.8861607313156128\n",
      "test: 0.8232421875\n",
      "train: 0.898809552192688\n",
      "test: 0.81640625\n",
      "train: 0.8816964030265808\n",
      "test: 0.8125\n",
      "train: 0.8928571343421936\n",
      "test: 0.810546875\n",
      "train: 0.8883928656578064\n",
      "test: 0.7998046875\n",
      "train: 0.8794642686843872\n",
      "test: 0.822265625\n",
      "train: 0.9002976417541504\n",
      "test: 0.8154296875\n",
      "train: 0.8757440447807312\n",
      "test: 0.8115234375\n",
      "train: 0.8839285969734192\n",
      "test: 0.77734375\n",
      "train: 0.883184552192688\n",
      "test: 0.787109375\n",
      "train: 0.8995535969734192\n",
      "test: 0.8125\n",
      "train: 0.8973214030265808\n",
      "test: 0.7861328125\n",
      "train: 0.886904776096344\n",
      "test: 0.818359375\n",
      "train: 0.8772321343421936\n",
      "test: 0.79296875\n",
      "train: 0.8913690447807312\n",
      "test: 0.8076171875\n",
      "train: 0.8936011791229248\n",
      "test: 0.814453125\n",
      "train: 0.898065447807312\n",
      "test: 0.8134765625\n",
      "train: 0.8898809552192688\n",
      "test: 0.7939453125\n",
      "train: 0.883184552192688\n",
      "test: 0.8017578125\n",
      "train: 0.8928571343421936\n",
      "test: 0.802734375\n",
      "train: 0.886904776096344\n",
      "test: 0.814453125\n",
      "train: 0.8965773582458496\n",
      "test: 0.802734375\n",
      "train: 0.8876488208770752\n",
      "test: 0.806640625\n",
      "train: 0.886904776096344\n",
      "test: 0.8056640625\n",
      "train: 0.8742559552192688\n",
      "test: 0.8232421875\n",
      "train: 0.8809523582458496\n",
      "test: 0.8046875\n",
      "train: 0.8958333134651184\n",
      "test: 0.806640625\n",
      "train: 0.8794642686843872\n",
      "test: 0.8017578125\n",
      "train: 0.8839285969734192\n",
      "test: 0.8017578125\n",
      "train: 0.9002976417541504\n",
      "test: 0.7978515625\n",
      "train: 0.8958333134651184\n",
      "test: 0.7890625\n",
      "train: 0.8854166865348816\n",
      "test: 0.8095703125\n",
      "train: 0.8973214030265808\n",
      "test: 0.8369140625\n",
      "train: 0.8921130895614624\n",
      "test: 0.8095703125\n",
      "train: 0.8764880895614624\n",
      "test: 0.810546875\n",
      "train: 0.8973214030265808\n",
      "test: 0.81640625\n",
      "train: 0.8928571343421936\n",
      "test: 0.8125\n",
      "train: 0.8764880895614624\n",
      "test: 0.814453125\n",
      "train: 0.8809523582458496\n",
      "test: 0.8251953125\n",
      "train: 0.9040178656578064\n",
      "test: 0.814453125\n",
      "train: 0.886904776096344\n",
      "test: 0.8134765625\n",
      "train: 0.9032738208770752\n",
      "test: 0.8212890625\n",
      "train: 0.8965773582458496\n",
      "test: 0.8349609375\n",
      "train: 0.8921130895614624\n",
      "test: 0.8447265625\n",
      "train: 0.890625\n",
      "test: 0.8212890625\n",
      "train: 0.8973214030265808\n",
      "test: 0.8232421875\n",
      "train: 0.890625\n",
      "test: 0.8388671875\n",
      "train: 0.898809552192688\n",
      "test: 0.81640625\n",
      "train: 0.90625\n",
      "test: 0.822265625\n",
      "train: 0.898065447807312\n",
      "test: 0.80859375\n",
      "train: 0.8958333134651184\n",
      "test: 0.8056640625\n",
      "train: 0.8913690447807312\n",
      "test: 0.8037109375\n",
      "train: 0.8973214030265808\n",
      "test: 0.8115234375\n",
      "train: 0.8839285969734192\n",
      "test: 0.767578125\n",
      "train: 0.8958333134651184\n",
      "test: 0.8916015625\n",
      "train: 0.8965773582458496\n",
      "test: 0.8115234375\n",
      "train: 0.8913690447807312\n",
      "test: 0.8037109375\n",
      "train: 0.894345223903656\n",
      "test: 0.8046875\n",
      "train: 0.8936011791229248\n",
      "test: 0.8349609375\n",
      "train: 0.890625\n",
      "test: 0.8115234375\n",
      "train: 0.8913690447807312\n",
      "test: 0.8056640625\n",
      "train: 0.9047619104385376\n",
      "test: 0.80078125\n",
      "train: 0.8816964030265808\n",
      "test: 0.8056640625\n",
      "train: 0.8876488208770752\n",
      "test: 0.8125\n",
      "train: 0.9002976417541504\n",
      "test: 0.81640625\n",
      "train: 0.898809552192688\n",
      "test: 0.830078125\n",
      "train: 0.898065447807312\n",
      "test: 0.8271484375\n",
      "train: 0.8883928656578064\n",
      "test: 0.8203125\n",
      "train: 0.8921130895614624\n",
      "test: 0.818359375\n",
      "train: 0.9010416865348816\n",
      "test: 0.8125\n",
      "train: 0.8913690447807312\n",
      "test: 0.8359375\n",
      "train: 0.886904776096344\n",
      "test: 0.830078125\n",
      "train: 0.9174107313156128\n",
      "test: 0.841796875\n",
      "train: 0.9040178656578064\n",
      "test: 0.8193359375\n",
      "train: 0.871279776096344\n",
      "test: 0.8076171875\n",
      "train: 0.8809523582458496\n",
      "test: 0.8330078125\n",
      "train: 0.8995535969734192\n",
      "test: 0.8173828125\n",
      "train: 0.8839285969734192\n",
      "test: 0.8037109375\n",
      "train: 0.8928571343421936\n",
      "test: 0.80859375\n",
      "train: 0.8891369104385376\n",
      "test: 0.814453125\n",
      "train: 0.8876488208770752\n",
      "test: 0.818359375\n",
      "train: 0.9047619104385376\n",
      "test: 0.841796875\n",
      "train: 0.9032738208770752\n",
      "test: 0.7822265625\n",
      "train: 0.886904776096344\n",
      "test: 0.81640625\n",
      "train: 0.875\n",
      "test: 0.8369140625\n",
      "train: 0.8846726417541504\n",
      "test: 0.81640625\n",
      "train: 0.8898809552192688\n",
      "test: 0.810546875\n",
      "train: 0.886904776096344\n",
      "test: 0.8095703125\n",
      "train: 0.8846726417541504\n",
      "test: 0.8232421875\n",
      "train: 0.8816964030265808\n",
      "test: 0.8046875\n",
      "train: 0.8742559552192688\n",
      "test: 0.8212890625\n",
      "train: 0.8921130895614624\n",
      "test: 0.8203125\n",
      "train: 0.8958333134651184\n",
      "test: 0.826171875\n",
      "train: 0.8928571343421936\n",
      "test: 0.8046875\n",
      "train: 0.8854166865348816\n",
      "test: 0.7900390625\n",
      "train: 0.8839285969734192\n",
      "test: 0.8544921875\n",
      "train: 0.8921130895614624\n",
      "test: 0.8779296875\n",
      "train: 0.8816964030265808\n",
      "test: 0.8427734375\n",
      "train: 0.8809523582458496\n",
      "test: 0.8330078125\n",
      "train: 0.8950892686843872\n",
      "test: 0.8720703125\n",
      "train: 0.898809552192688\n",
      "test: 0.8359375\n",
      "train: 0.8898809552192688\n",
      "test: 0.8427734375\n"
     ]
    }
   ],
   "source": [
    "# train eval loop\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "accs_train_outer = []\n",
    "accs_test_outer = []\n",
    "for e in range(200):\n",
    "    model.train()\n",
    "    accs_train = []\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, y = batch\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out[\"loss\"]\n",
    "        # train acc is here not calculated properly for simplification\n",
    "        accs_train.append(((torch.argmax(out[\"logits\"], dim=1) == y).sum() / batch_size).item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 20:\n",
    "            break\n",
    "    print(\"train:\", torch.tensor(accs_train).mean().item())\n",
    "    accs_train_outer.append(torch.tensor(accs_train).mean().item())\n",
    "    model.eval()\n",
    "    accs_test = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader_test):\n",
    "            x, y = batch\n",
    "            out = model(input_ids=x, labels=y)\n",
    "            accs_test.append(((torch.argmax(out[\"logits\"], dim=1) == y).sum() / batch_size).item())\n",
    "    print(\"test:\", torch.tensor(accs_test).mean().item())\n",
    "    accs_test_outer.append(torch.tensor(accs_test).mean().item())\n",
    "    torch.save(model, f\"model{e}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. use gradcam to deduct instance level prediction from bag level prediction \n",
    "- bag level: sentence contains typo y/n\n",
    "- instance level: word contains typo y/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(tensor: torch.Tensor, thresh: float) -> torch.Tensor:\n",
    "    return tensor > thresh\n",
    "\n",
    "def greater_zero_or_none(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    if tensor is not None:\n",
    "        return mask(tensor=tensor, thresh=0).int() * tensor\n",
    "    \n",
    "def rectify_hook(\n",
    "        module: torch.nn.Module,\n",
    "        grad_inputs: torch.Tensor,\n",
    "        grad_outputs: torch.Tensor):\n",
    "    new_grad_inputs = tuple([greater_zero_or_none(gi) for gi in grad_inputs])\n",
    "    return new_grad_inputs\n",
    "\n",
    "\n",
    "def display(text: str):\n",
    "    # load model and tokenizer and prepare for calculating gradcam on embeddings\n",
    "    model: BertForSequenceClassification = torch.load(\"model199.pt\")\n",
    "    for module in model.modules():\n",
    "        module.register_backward_hook(rectify_hook)\n",
    "    tok = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(tok(text)[\"input_ids\"]).unsqueeze(0)\n",
    "    e: torch.Tensor = model.bert.embeddings(input_ids)\n",
    "    e.requires_grad_(True)\n",
    "    e.retain_grad()\n",
    "\n",
    "    # calculate gradcam\n",
    "    logits = model.forward(inputs_embeds=e)[\"logits\"]\n",
    "    pred = np.round(torch.nn.functional.softmax(logits)[0][0].item(), 2)\n",
    "    logit = logits[0][0]\n",
    "    logit.backward()\n",
    "    grad = torch.mean(e.grad.data, dim=2).squeeze()\n",
    "    grad = grad[1:-1]\n",
    "    grad = grad + grad.min() * -1\n",
    "    grad = grad / grad.sum()\n",
    "\n",
    "    # merge tokens into words and sum up gradcam per word\n",
    "    tokens = tok.tokenize(text)\n",
    "    tokens_clean = [tokens[i] if tokens[i+1][:2] == \"##\" else tokens[i] + \" \" for i in range(len(tokens) - 1)]\n",
    "    tokens_clean.append(tokens[-1])\n",
    "    tokens_clean = [t[2:] if t[:2] == \"##\" else t for t in tokens_clean]\n",
    "    word_grads = []\n",
    "    words = []\n",
    "    grad_val = 0\n",
    "    word = \"\"\n",
    "    for i in range(len(tokens_clean)):\n",
    "        if tokens_clean[i][-1] == \" \":\n",
    "            word += tokens_clean[i]\n",
    "            grad_val += grad[i]\n",
    "            word_grads.append(grad_val)\n",
    "            words.append(word)\n",
    "            word = \"\"\n",
    "            grad_val = 0\n",
    "        else:\n",
    "            word += tokens_clean[i]\n",
    "            grad_val += grad[i]\n",
    "            if i == (len(tokens_clean) - 1):\n",
    "                words.append(word)\n",
    "                word_grads.append(grad_val)\n",
    "    words = [w[:-1] if w[-1] == \" \" else w for w in words]\n",
    "    word_grads = [g.item() for g in word_grads]\n",
    "    word_grads = np.array(word_grads).reshape(1, -1)\n",
    "\n",
    "    # plot results\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    heatmap = ax.imshow(word_grads.reshape(1, -1), cmap='Reds', aspect='auto')\n",
    "\n",
    "    # Adding labels within each cell\n",
    "    for i, label in enumerate(words):\n",
    "        ax.text(i, 0, label, ha='center', va='center', color='black')\n",
    "\n",
    "    ax.set_title(f\"word-level gradcam\\nprediction: sentence contains typo (confidence: {str(pred)})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.savefig(f\"gradcam.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_15928\\4029962037.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = np.round(torch.nn.functional.softmax(logits)[0][0].item(), 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Die Zwiebeln udn das Ei in eine Schüssel geben.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_machine_learning_projects_light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\anaconda3\\envs\\small_machine_learning_projects_light\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# phonetics\n",
    "import epitran  \n",
    "import cologne_phonetics\n",
    "from phonetisch import soundex\n",
    "from abydos.phonetic import Phonet, Haase, RethSchek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. create sentences with common typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringManipulator:\n",
    "    @staticmethod\n",
    "    def manipulate_words(string: str, func, min_p: float = 0.1, max_p: float = 0.5) -> str:\n",
    "        \"\"\"\n",
    "        manipulate each words with a probability between min_p and max_p\n",
    "        \"\"\"\n",
    "        words = string.split(\" \")\n",
    "        # determine which words to manipulate\n",
    "        p = random.uniform(min_p, max_p)\n",
    "        pos_manipulate = [i for i in range(len(words)) if random.random() <= p]\n",
    "        # manipulate\n",
    "        string_final = \"\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word != \"\":\n",
    "                if i in pos_manipulate:\n",
    "                    string_final += \" \" + random.choice(func(word))\n",
    "                else:\n",
    "                    string_final += \" \" + word\n",
    "        return string_final[1:]\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_capitalizations(s: str) -> List[str]:\n",
    "        results = []\n",
    "        max_num = 1 << len(s)  # 2^len(s)\n",
    "        for i in range(max_num):\n",
    "            word = []\n",
    "            for j in range(len(s)):\n",
    "                if i & (1 << j):\n",
    "                    word.append(s[j].upper())\n",
    "                else:\n",
    "                    word.append(s[j].lower())\n",
    "            results.append(''.join(word))\n",
    "        return results\n",
    "\n",
    "    def replace_group(self, string: str, group_members: List[str], adapt_capitalization: bool = \"first\"):\n",
    "        # for adapting capitalization\n",
    "        if adapt_capitalization != False:\n",
    "            group_members_capitalized = []\n",
    "            for m in group_members:\n",
    "                group_members_capitalized.extend(self.generate_capitalizations(m))\n",
    "        else:\n",
    "            group_members_capitalized = group_members\n",
    "        # to_repl -> repl_with\n",
    "        variants = []\n",
    "        for to_repl in group_members_capitalized:\n",
    "            to_repl = re.sub(pattern=\"\\.\", repl=\"\\.\", string=to_repl)\n",
    "            for repl_with in group_members:\n",
    "                if repl_with != to_repl:\n",
    "                    # adapt capitalization\n",
    "                    if adapt_capitalization == \"first\":\n",
    "                        if to_repl[0].isupper():\n",
    "                            repl_with = list(repl_with)\n",
    "                            repl_with[0] = repl_with[0].upper()\n",
    "                            repl_with = \"\".join(repl_with)\n",
    "                    elif adapt_capitalization == \"auto\":\n",
    "                        if to_repl[0].isupper() and to_repl[1].isupper():\n",
    "                            repl_with = repl_with.upper()\n",
    "                        elif to_repl[0].isupper():\n",
    "                            repl_with = list(repl_with)\n",
    "                            repl_with[0] = repl_with[0].upper()\n",
    "                            repl_with = \"\".join(repl_with)\n",
    "                    # create variant\n",
    "                    variant = re.sub(pattern=to_repl, repl=repl_with, string=string)\n",
    "                    variants.append(variant)\n",
    "        return variants\n",
    "\n",
    "\n",
    "class Typo(StringManipulator):\n",
    "    \"\"\"\n",
    "    uses german keyboard layout by default (only letters)\n",
    "    \"\"\"\n",
    "    def __init__(self, alternative_layout: dict = None) -> None:\n",
    "        super().__init__()\n",
    "        self.epi = epitran.Epitran('deu-Latn')\n",
    "        self.phonet = Phonet()\n",
    "        self.haase = Haase()\n",
    "        self.rethshek = RethSchek()\n",
    "        if alternative_layout is not None:\n",
    "            layout = alternative_layout\n",
    "        else:\n",
    "            layout = {\n",
    "            'q': (0, 0), 'w': (0, 1), 'e': (0, 2), 'r': (0, 3), 't': (0, 4), 'z': (0, 5), 'u': (0, 6), 'i': (0, 7), 'o': (0, 8), 'p': (0, 9), 'ü': (0, 10),\n",
    "            'a': (1, 0), 's': (1, 1), 'd': (1, 2), 'f': (1, 3), 'g': (1, 4), 'h': (1, 5), 'j': (1, 6), 'k': (1, 7), 'l': (1, 8), 'ö': (1, 9), 'ä': (1, 10),\n",
    "            'y': (2, 0), 'x': (2, 1), 'c': (2, 2), 'v': (2, 3), 'b': (2, 4), 'n': (2, 5), 'm': (2, 6)}\n",
    "        # euclidian distance\n",
    "        coords = np.array(list(layout.values()))\n",
    "        dist_matrix = np.sqrt(np.sum((coords[:, np.newaxis, :] - coords[np.newaxis, :, :]) ** 2, axis=-1))\n",
    "        for i in range(len(dist_matrix)):\n",
    "            dist_matrix[i][i] = np.inf  # main diagonal = inf to ignore replacing key with itself\n",
    "        self.dist_df = pd.DataFrame(data=dist_matrix, columns=list(layout.keys()), index=list(layout.keys()))\n",
    "\n",
    "    @staticmethod\n",
    "    def lastvowels_positions(string: str) -> List[int]:\n",
    "        return [v.end() -1 for v in re.finditer(pattern=\"[aeiouäöüAEIOUÄÖÜ]+\", string=string)]\n",
    "    \n",
    "    def german_phonetic_similarity(self, str1: str, str2: str) -> str:\n",
    "        epitran = self.epi.transliterate(str1) == self.epi.transliterate(str2)\n",
    "        cologne = \" \".join([p[1] for p in cologne_phonetics.encode(str1)]) == \" \".join([p[1] for p in cologne_phonetics.encode(str2)])\n",
    "        phonetisch = soundex.encode_word(str1) == soundex.encode_word(str2)\n",
    "        haase = self.haase.encode(str1) == self.haase.encode(str2)\n",
    "        rethschek = self.rethshek.encode(str1) == self.rethshek.encode(str2)\n",
    "        phonet = self.phonet.encode(str1) == self.phonet.encode(str2)\n",
    "        sum = int(epitran) + int(cologne) + int(phonetisch) + int(phonet) + int(haase) + int(rethschek)\n",
    "        return sum > 3\n",
    "\n",
    "    def all(self, string: str, balanced: bool = True) -> List[str]:\n",
    "        d = self.deletion(string)\n",
    "        t = self.transposition(string)\n",
    "        i = self.insertion(string)\n",
    "        ki = self.keyboard_substitution(string=string)\n",
    "        p = self.phonetics(string)\n",
    "        if balanced:\n",
    "            k = max(min(len(d), len(t), len(i), len(ki), len(p)), 1)\n",
    "            if len(d) != 0:\n",
    "                d = random.choices(d, k=k)\n",
    "            if len(t) != 0:\n",
    "                t = random.choices(t, k=k)\n",
    "            if len(i) != 0:\n",
    "                i = random.choices(i, k=k)\n",
    "            if len(ki) != 0:\n",
    "                ki = random.choices(ki, k=k)\n",
    "            if len(p) != 0:\n",
    "                p = random.choices(p, k=k)\n",
    "        return list(set(d + t + i + ki + p))\n",
    "\n",
    "    def deletion(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string)):\n",
    "            variants.append(string[:pos] + string[(pos + 1):])\n",
    "        return list(set(variants))\n",
    "    \n",
    "    def transposition(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string) - 1):\n",
    "            if (pos + 2) < len(string):\n",
    "                variants.append(string[:pos] + string[pos + 1] + string[pos] + string[(pos + 2):])\n",
    "            else:\n",
    "                variants.append(string[:pos] + string[pos + 1] + string[pos])\n",
    "        if string in variants:\n",
    "            variants.remove(string)\n",
    "        return variants\n",
    "    \n",
    "    def insertion(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for pos in range(len(string)):\n",
    "            chars = self.keyboard_substitution(string=string[pos])\n",
    "            chars.append(string[pos])\n",
    "            for char in chars:\n",
    "                variants.append(string[:pos] + char + string[pos:])\n",
    "        return list(set(variants))\n",
    "\n",
    "    def keyboard_substitution(self, string: str, max_dist: float = 1, char_positions: List[int] = None) -> List[str]:\n",
    "        if char_positions is None:\n",
    "            char_positions = range(len(string))\n",
    "        variants = []\n",
    "        for pos in char_positions:\n",
    "            char = string[pos]\n",
    "            if char in self.dist_df.index:\n",
    "                repl_with = self.dist_df[char][self.dist_df[char] <= max_dist].index\n",
    "                variants.extend([string[:pos] + c + string[(pos + 1):] for c in repl_with])\n",
    "        return variants\n",
    "    \n",
    "    def phonetics(self, string: str) -> List[str]:\n",
    "        def insert_h(string: str, match_nrs: int = None):\n",
    "            if match_nrs is None:\n",
    "                match_nrs = range(len(string))\n",
    "            string_out = \"\"\n",
    "            match_nr = -1\n",
    "            pos_lasts = self.lastvowels_positions(string)\n",
    "            for i in range(len(string)):\n",
    "                string_out += string[i]\n",
    "                if i in pos_lasts:\n",
    "                    if i != 0:\n",
    "                        if string[i] != string[i - 1]:\n",
    "                            if len(string) >= (i + 3):\n",
    "                                if (string[i + 1] != string[i + 2]) and (string[i+1:i+2] != \"dt\"):\n",
    "                                    match_nr += 1\n",
    "                                    if match_nr in match_nrs:\n",
    "                                        string_out += \"h\"\n",
    "                            else:\n",
    "                                match_nr +=1\n",
    "                                if match_nr in match_nrs:\n",
    "                                    string_out += \"h\"\n",
    "                    else:\n",
    "                        match_nr += 1\n",
    "                        if match_nr in match_nrs:\n",
    "                            string_out += \"h\"\n",
    "            return string_out\n",
    "        \n",
    "        variants = []\n",
    "        # h after vowel\n",
    "        for i in range(len(self.lastvowels_positions(string))):\n",
    "            variants.append(insert_h(string=string, match_nrs=[i]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ß\", \"s\", \"ss\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"dt\", \"tt\", \"t\", \"d\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ld\", \"ld\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"t\", \"th\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ai\", \"ei\", \"ay\", \"ey\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"er\", \"a\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"x\", \"ks\", \"cks\", \"chs\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"k\", \"ck\", \"c\", \"q\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"gk\", \"k\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"b\", \"p\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"z\", \"ts\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"g\", \"j\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ph\", \"f\", \"v\", \"pf\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ie\", \"i\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"eu\", \"oi\", \"oy\", \"äu\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ts\", \"ds\", \"tz\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"e\", \"ä\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"i\", \"j\", \"y\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ü\", \"y\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ig\", \"ich\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"qu\", \"kv\", \"kw\"]))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"tsch\", \"dsch\", \"dj\", \"tj\", \"zsch\", \"tzsch\"]))\n",
    "\n",
    "        variants = list(set(variants))\n",
    "        variants.remove(string)\n",
    "        variants_clean = []\n",
    "        for v in variants:\n",
    "            if self.german_phonetic_similarity(v, string):\n",
    "                variants_clean.append(v)\n",
    "        return variants_clean\n",
    "\n",
    "\n",
    "class OCR(StringManipulator):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def whitespace(string: str) -> List[str]:\n",
    "        variants = []\n",
    "        for i in range(1, len(string)):\n",
    "            variants.append(string[:i] + \" \" + string[i:])\n",
    "        return variants\n",
    "\n",
    "    @staticmethod\n",
    "    def fragment(string: str) -> List[str]:\n",
    "        \"\"\"especially for badly scanned docs, small fragments might be misinterpreted as chars\"\"\"\n",
    "        variants = []\n",
    "        for fragment in \".,'`´\":\n",
    "            for i in range(len(string)):\n",
    "                variants.append(string[:i] + fragment + string[i:])\n",
    "        return variants\n",
    "\n",
    "    def look(self, string: str) -> List[str]:\n",
    "        variants = []\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"rn\", \"m\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"e\", \"c\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"O\", \"0\", \"o\", \"Ö\", \"ö\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"V\", \"U\", \"v\", \"u\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\".\", \",\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\":\", \";\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"!\", \"I\", \"1\", \"l\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"n\", \"h\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"S\", \"s\", \"5\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"B\", \"8\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"vv\", \"w\", \"VV\", \"W\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"ij\", \"u\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"cl\", \"d\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"-\", \"_\"], adapt_capitalization=False))\n",
    "        variants.extend(self.replace_group(string=string, group_members=[\"'\", \"`\", \"´\", \"°\"], adapt_capitalization=False))\n",
    "        variants = list(set(variants))\n",
    "        variants.remove(string) \n",
    "        return variants\n",
    "    \n",
    "    def all(self, string: str, balanced: bool = True, p_w: float = 0.15, p_f: float = 0.15) -> List[str]:\n",
    "        p_l = 1 - (p_w + p_f)  # TODO implement probability based\n",
    "        w = self.whitespace(string)\n",
    "        f = self.fragment(string)\n",
    "        l = self.look(string)\n",
    "        if balanced:\n",
    "            k = max(min(len(w), len(f), len(l)), 1)\n",
    "            w = random.choices(w, k=k)\n",
    "            f = random.choices(f, k=k)\n",
    "            l = random.choices(l, k=k)\n",
    "        return list(set(w + f + l))\n",
    "    \n",
    "\n",
    "def check_for_abbreviations(string: str):\n",
    "    matches = re.finditer(pattern=\"\\w+\\.\\s\", string=string)\n",
    "    match_strs = []\n",
    "    for match in matches:\n",
    "        match_strs.append(string[match.start(): match.end()])\n",
    "    print(pd.Series(match_strs).value_counts().head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"recipes.json\", \"rb\") as f:\n",
    "    j = json.load(f)\n",
    "instructions = \". \".join(list(set([j[i][\"Instructions\"] for i in range(len(j))])))\n",
    "\n",
    "\n",
    "# delete . from abbreviations to allow splitting per .\n",
    "instructions = re.sub(pattern=\"ca\\.\", repl=\"ca\", string=instructions)\n",
    "instructions = re.sub(pattern=\"Min\\.\", repl=\"Min\", string=instructions)\n",
    "instructions = re.sub(pattern=\"min\\.\", repl=\"min\", string=instructions)\n",
    "instructions = re.sub(pattern=\"evtl\\.\", repl=\"evtl\", string=instructions)\n",
    "instructions = re.sub(pattern=\"bzw\\.\", repl=\"bzw\", string=instructions)\n",
    "instructions = re.sub(pattern=\"Ca\\.\", repl=\"ca\", string=instructions)\n",
    "instructions = re.sub(pattern=\"B\\.\", repl=\"B\", string=instructions)\n",
    "instructions = re.sub(pattern=\"[0-9]\\.\", repl=\"\", string=instructions)\n",
    "sentences = pd.Series([s.strip() for s in instructions.split(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom german_spelling import Typo\\n\\ntypo = Typo()\\nversions = []\\nfor i in range(36):\\n    print(\"version\", i)\\n    sentences_typo = []\\n    for j, sentence in enumerate(sentences):\\n        sentences_typo.append(typo.manipulate_words(string=sentence, min_p=0.1, max_p=0.3, func=typo.all))\\n    pd.Series(sentences_typo).to_frame().to_csv(f\"recipe_variants/{i}.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used to generate faulty sentences\n",
    "\"\"\"\n",
    "typo = Typo()\n",
    "versions = []\n",
    "for i in range(36):\n",
    "    print(\"version\", i)\n",
    "    sentences_typo = []\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        sentences_typo.append(typo.manipulate_words(string=sentence, min_p=0.1, max_p=0.3, func=typo.all))\n",
    "    pd.Series(sentences_typo).to_frame().to_csv(f\"recipe_variants/{i}.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train sequence classifier to detect sentences with typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_tensor[idx], self.target_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "# load original sentences and faulty sentences and split them into train and test sets\n",
    "variants = []\n",
    "for i in range(36):\n",
    "    variants.append(pd.read_csv(f\"recipe_variants/{i}.csv\"))\n",
    "variants = pd.concat(variants, axis=1)\n",
    "\n",
    "variants.columns = [str(i) for i in range(36)]\n",
    "variants.eq(sentences, axis=0).any(axis=1).mean()\n",
    "for col in variants.columns:\n",
    "    variants[col] = variants[col].where(variants[col] != sentences, \"\")\n",
    "\n",
    "variants = variants[sentences != \"\"].dropna()\n",
    "negatives_train = variants.head(30_000)\n",
    "negatives_test = variants[~variants.index.isin(negatives_train.index)]\n",
    "negatives_train = negatives_train.values.reshape(30_000 * 36)\n",
    "negatives_train = negatives_train[negatives_train != \"\"]\n",
    "negatives_test = negatives_test.values.reshape(13746 * 36)\n",
    "negatives_test = negatives_test[negatives_test != \"\"]\n",
    "positives_train = sentences.head(30_000)\n",
    "positives_test = sentences[~sentences.index.isin(positives_train.index)]\n",
    "negatives_train = pd.Series(negatives_train).sample(frac=1).values[:len(positives_train)]\n",
    "negatives_test = pd.Series(negatives_train).sample(frac=1).values[:len(positives_test)]\n",
    "\n",
    "# tokenize sentences to form input ids (usually it is recommended to use masking)\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "negatives_train_tokens = torch.tensor(tok(list(negatives_train), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "positives_train_tokens = torch.tensor(tok(list(positives_train), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "\n",
    "negatives_test_tokens = torch.tensor(tok(list(negatives_test), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "positives_test_tokens = torch.tensor(tok(list(positives_test), truncation=True, max_length=64, padding=\"max_length\")[\"input_ids\"])\n",
    "\n",
    "inputs_train = torch.cat([negatives_train_tokens, positives_train_tokens])\n",
    "inputs_test = torch.cat([negatives_test_tokens, positives_test_tokens])\n",
    "\n",
    "# create target tensors\n",
    "train_target = torch.ones(len(negatives_train_tokens) + len(positives_train_tokens), dtype=int)\n",
    "train_target[:len(negatives_train_tokens)] = 0\n",
    "test_target = torch.ones(len(negatives_test_tokens) + len(positives_test_tokens), dtype=int)\n",
    "test_target[:len(negatives_test_tokens)] = 0\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "dataset_train = CustomDataset(inputs_train, train_target)\n",
    "dataset_test = CustomDataset(inputs_test[:1024], test_target[:1024])\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.590029776096344\n",
      "test: 0.77734375\n",
      "train: 0.8221726417541504\n",
      "test: 0.70703125\n",
      "train: 0.8288690447807312\n",
      "test: 0.748046875\n",
      "train: 0.8563988208770752\n",
      "test: 0.7734375\n",
      "train: 0.8526785969734192\n",
      "test: 0.7783203125\n",
      "train: 0.8623511791229248\n",
      "test: 0.79296875\n",
      "train: 0.8549107313156128\n",
      "test: 0.7265625\n",
      "train: 0.8571428656578064\n",
      "test: 0.736328125\n",
      "train: 0.8333333134651184\n",
      "test: 0.7529296875\n",
      "train: 0.883184552192688\n",
      "test: 0.7578125\n",
      "train: 0.8459821343421936\n",
      "test: 0.8076171875\n",
      "train: 0.8422619104385376\n",
      "test: 0.7041015625\n",
      "train: 0.8623511791229248\n",
      "test: 0.765625\n",
      "train: 0.882440447807312\n",
      "test: 0.7578125\n",
      "train: 0.8616071343421936\n",
      "test: 0.8212890625\n",
      "train: 0.8534226417541504\n",
      "test: 0.7060546875\n",
      "train: 0.866815447807312\n",
      "test: 0.7744140625\n",
      "train: 0.8623511791229248\n",
      "test: 0.763671875\n",
      "train: 0.8623511791229248\n",
      "test: 0.7353515625\n",
      "train: 0.875\n",
      "test: 0.783203125\n",
      "train: 0.8563988208770752\n",
      "test: 0.7646484375\n",
      "train: 0.8809523582458496\n",
      "test: 0.7919921875\n",
      "train: 0.8482142686843872\n",
      "test: 0.7939453125\n",
      "train: 0.8683035969734192\n",
      "test: 0.775390625\n",
      "train: 0.8742559552192688\n",
      "test: 0.7626953125\n",
      "train: 0.8757440447807312\n",
      "test: 0.80078125\n",
      "train: 0.840029776096344\n",
      "test: 0.7734375\n",
      "train: 0.882440447807312\n",
      "test: 0.8037109375\n",
      "train: 0.8742559552192688\n",
      "test: 0.779296875\n",
      "train: 0.8422619104385376\n",
      "test: 0.763671875\n",
      "train: 0.863095223903656\n",
      "test: 0.7880859375\n",
      "train: 0.8876488208770752\n",
      "test: 0.802734375\n",
      "train: 0.8816964030265808\n",
      "test: 0.779296875\n",
      "train: 0.8802083134651184\n",
      "test: 0.765625\n",
      "train: 0.8690476417541504\n",
      "test: 0.7890625\n",
      "train: 0.8764880895614624\n",
      "test: 0.767578125\n",
      "train: 0.8623511791229248\n",
      "test: 0.791015625\n",
      "train: 0.859375\n",
      "test: 0.7568359375\n",
      "train: 0.8653273582458496\n",
      "test: 0.7822265625\n",
      "train: 0.8720238208770752\n",
      "test: 0.7548828125\n",
      "train: 0.867559552192688\n",
      "test: 0.7578125\n",
      "train: 0.8757440447807312\n",
      "test: 0.794921875\n",
      "train: 0.8720238208770752\n",
      "test: 0.8115234375\n",
      "train: 0.8720238208770752\n",
      "test: 0.76171875\n",
      "train: 0.8638392686843872\n",
      "test: 0.7978515625\n",
      "train: 0.8913690447807312\n",
      "test: 0.787109375\n",
      "train: 0.8653273582458496\n",
      "test: 0.802734375\n",
      "train: 0.8727678656578064\n",
      "test: 0.771484375\n",
      "train: 0.8660714030265808\n",
      "test: 0.7919921875\n",
      "train: 0.8735119104385376\n",
      "test: 0.7587890625\n",
      "train: 0.8727678656578064\n",
      "test: 0.8076171875\n",
      "train: 0.8757440447807312\n",
      "test: 0.77734375\n",
      "train: 0.8645833134651184\n",
      "test: 0.787109375\n",
      "train: 0.8809523582458496\n",
      "test: 0.7978515625\n",
      "train: 0.8683035969734192\n",
      "test: 0.8154296875\n",
      "train: 0.8638392686843872\n",
      "test: 0.79296875\n",
      "train: 0.8601190447807312\n",
      "test: 0.80078125\n",
      "train: 0.8623511791229248\n",
      "test: 0.8076171875\n",
      "train: 0.8794642686843872\n",
      "test: 0.8037109375\n",
      "train: 0.8876488208770752\n",
      "test: 0.7900390625\n",
      "train: 0.8727678656578064\n",
      "test: 0.8095703125\n",
      "train: 0.8809523582458496\n",
      "test: 0.7900390625\n",
      "train: 0.882440447807312\n",
      "test: 0.8154296875\n",
      "train: 0.8913690447807312\n",
      "test: 0.7958984375\n",
      "train: 0.886904776096344\n",
      "test: 0.8017578125\n",
      "train: 0.8898809552192688\n",
      "test: 0.8212890625\n",
      "train: 0.902529776096344\n",
      "test: 0.810546875\n",
      "train: 0.8883928656578064\n",
      "test: 0.80078125\n",
      "train: 0.883184552192688\n",
      "test: 0.8056640625\n",
      "train: 0.890625\n",
      "test: 0.7998046875\n",
      "train: 0.886904776096344\n",
      "test: 0.826171875\n",
      "train: 0.8764880895614624\n",
      "test: 0.8095703125\n",
      "train: 0.882440447807312\n",
      "test: 0.798828125\n",
      "train: 0.8898809552192688\n",
      "test: 0.810546875\n",
      "train: 0.8936011791229248\n",
      "test: 0.796875\n",
      "train: 0.9092261791229248\n",
      "test: 0.814453125\n",
      "train: 0.890625\n",
      "test: 0.8076171875\n",
      "train: 0.8854166865348816\n",
      "test: 0.8154296875\n",
      "train: 0.902529776096344\n",
      "test: 0.8251953125\n",
      "train: 0.8965773582458496\n",
      "test: 0.8056640625\n",
      "train: 0.8772321343421936\n",
      "test: 0.8125\n",
      "train: 0.8690476417541504\n",
      "test: 0.791015625\n",
      "train: 0.8720238208770752\n",
      "test: 0.7998046875\n",
      "train: 0.886904776096344\n",
      "test: 0.787109375\n",
      "train: 0.882440447807312\n",
      "test: 0.794921875\n",
      "train: 0.8839285969734192\n",
      "test: 0.80078125\n",
      "train: 0.8854166865348816\n",
      "test: 0.841796875\n",
      "train: 0.8697916865348816\n",
      "test: 0.806640625\n",
      "train: 0.8757440447807312\n",
      "test: 0.8056640625\n",
      "train: 0.8846726417541504\n",
      "test: 0.794921875\n",
      "train: 0.8764880895614624\n",
      "test: 0.798828125\n",
      "train: 0.8846726417541504\n",
      "test: 0.794921875\n",
      "train: 0.8764880895614624\n",
      "test: 0.7939453125\n",
      "train: 0.8839285969734192\n",
      "test: 0.802734375\n",
      "train: 0.8839285969734192\n",
      "test: 0.8720703125\n",
      "train: 0.8928571343421936\n",
      "test: 0.7958984375\n",
      "train: 0.882440447807312\n",
      "test: 0.806640625\n",
      "train: 0.8965773582458496\n",
      "test: 0.7998046875\n",
      "train: 0.8861607313156128\n",
      "test: 0.8232421875\n",
      "train: 0.898809552192688\n",
      "test: 0.81640625\n",
      "train: 0.8816964030265808\n",
      "test: 0.8125\n",
      "train: 0.8928571343421936\n",
      "test: 0.810546875\n",
      "train: 0.8883928656578064\n",
      "test: 0.7998046875\n",
      "train: 0.8794642686843872\n",
      "test: 0.822265625\n",
      "train: 0.9002976417541504\n",
      "test: 0.8154296875\n",
      "train: 0.8757440447807312\n",
      "test: 0.8115234375\n",
      "train: 0.8839285969734192\n",
      "test: 0.77734375\n",
      "train: 0.883184552192688\n",
      "test: 0.787109375\n",
      "train: 0.8995535969734192\n",
      "test: 0.8125\n",
      "train: 0.8973214030265808\n",
      "test: 0.7861328125\n",
      "train: 0.886904776096344\n",
      "test: 0.818359375\n",
      "train: 0.8772321343421936\n",
      "test: 0.79296875\n",
      "train: 0.8913690447807312\n",
      "test: 0.8076171875\n",
      "train: 0.8936011791229248\n",
      "test: 0.814453125\n",
      "train: 0.898065447807312\n",
      "test: 0.8134765625\n",
      "train: 0.8898809552192688\n",
      "test: 0.7939453125\n",
      "train: 0.883184552192688\n",
      "test: 0.8017578125\n",
      "train: 0.8928571343421936\n",
      "test: 0.802734375\n",
      "train: 0.886904776096344\n",
      "test: 0.814453125\n",
      "train: 0.8965773582458496\n",
      "test: 0.802734375\n",
      "train: 0.8876488208770752\n",
      "test: 0.806640625\n",
      "train: 0.886904776096344\n",
      "test: 0.8056640625\n",
      "train: 0.8742559552192688\n",
      "test: 0.8232421875\n",
      "train: 0.8809523582458496\n",
      "test: 0.8046875\n",
      "train: 0.8958333134651184\n",
      "test: 0.806640625\n",
      "train: 0.8794642686843872\n",
      "test: 0.8017578125\n",
      "train: 0.8839285969734192\n",
      "test: 0.8017578125\n",
      "train: 0.9002976417541504\n",
      "test: 0.7978515625\n",
      "train: 0.8958333134651184\n",
      "test: 0.7890625\n",
      "train: 0.8854166865348816\n",
      "test: 0.8095703125\n",
      "train: 0.8973214030265808\n",
      "test: 0.8369140625\n",
      "train: 0.8921130895614624\n",
      "test: 0.8095703125\n",
      "train: 0.8764880895614624\n",
      "test: 0.810546875\n",
      "train: 0.8973214030265808\n",
      "test: 0.81640625\n",
      "train: 0.8928571343421936\n",
      "test: 0.8125\n",
      "train: 0.8764880895614624\n",
      "test: 0.814453125\n",
      "train: 0.8809523582458496\n",
      "test: 0.8251953125\n",
      "train: 0.9040178656578064\n",
      "test: 0.814453125\n",
      "train: 0.886904776096344\n",
      "test: 0.8134765625\n",
      "train: 0.9032738208770752\n",
      "test: 0.8212890625\n",
      "train: 0.8965773582458496\n",
      "test: 0.8349609375\n",
      "train: 0.8921130895614624\n",
      "test: 0.8447265625\n",
      "train: 0.890625\n",
      "test: 0.8212890625\n",
      "train: 0.8973214030265808\n",
      "test: 0.8232421875\n",
      "train: 0.890625\n",
      "test: 0.8388671875\n",
      "train: 0.898809552192688\n",
      "test: 0.81640625\n",
      "train: 0.90625\n",
      "test: 0.822265625\n",
      "train: 0.898065447807312\n",
      "test: 0.80859375\n",
      "train: 0.8958333134651184\n",
      "test: 0.8056640625\n",
      "train: 0.8913690447807312\n",
      "test: 0.8037109375\n",
      "train: 0.8973214030265808\n",
      "test: 0.8115234375\n",
      "train: 0.8839285969734192\n",
      "test: 0.767578125\n",
      "train: 0.8958333134651184\n",
      "test: 0.8916015625\n",
      "train: 0.8965773582458496\n",
      "test: 0.8115234375\n",
      "train: 0.8913690447807312\n",
      "test: 0.8037109375\n",
      "train: 0.894345223903656\n",
      "test: 0.8046875\n",
      "train: 0.8936011791229248\n",
      "test: 0.8349609375\n",
      "train: 0.890625\n",
      "test: 0.8115234375\n",
      "train: 0.8913690447807312\n",
      "test: 0.8056640625\n",
      "train: 0.9047619104385376\n",
      "test: 0.80078125\n",
      "train: 0.8816964030265808\n",
      "test: 0.8056640625\n",
      "train: 0.8876488208770752\n",
      "test: 0.8125\n",
      "train: 0.9002976417541504\n",
      "test: 0.81640625\n",
      "train: 0.898809552192688\n",
      "test: 0.830078125\n",
      "train: 0.898065447807312\n",
      "test: 0.8271484375\n",
      "train: 0.8883928656578064\n",
      "test: 0.8203125\n",
      "train: 0.8921130895614624\n",
      "test: 0.818359375\n",
      "train: 0.9010416865348816\n",
      "test: 0.8125\n",
      "train: 0.8913690447807312\n",
      "test: 0.8359375\n",
      "train: 0.886904776096344\n",
      "test: 0.830078125\n",
      "train: 0.9174107313156128\n",
      "test: 0.841796875\n",
      "train: 0.9040178656578064\n",
      "test: 0.8193359375\n",
      "train: 0.871279776096344\n",
      "test: 0.8076171875\n",
      "train: 0.8809523582458496\n",
      "test: 0.8330078125\n",
      "train: 0.8995535969734192\n",
      "test: 0.8173828125\n",
      "train: 0.8839285969734192\n",
      "test: 0.8037109375\n",
      "train: 0.8928571343421936\n",
      "test: 0.80859375\n",
      "train: 0.8891369104385376\n",
      "test: 0.814453125\n",
      "train: 0.8876488208770752\n",
      "test: 0.818359375\n",
      "train: 0.9047619104385376\n",
      "test: 0.841796875\n",
      "train: 0.9032738208770752\n",
      "test: 0.7822265625\n",
      "train: 0.886904776096344\n",
      "test: 0.81640625\n",
      "train: 0.875\n",
      "test: 0.8369140625\n",
      "train: 0.8846726417541504\n",
      "test: 0.81640625\n",
      "train: 0.8898809552192688\n",
      "test: 0.810546875\n",
      "train: 0.886904776096344\n",
      "test: 0.8095703125\n",
      "train: 0.8846726417541504\n",
      "test: 0.8232421875\n",
      "train: 0.8816964030265808\n",
      "test: 0.8046875\n",
      "train: 0.8742559552192688\n",
      "test: 0.8212890625\n",
      "train: 0.8921130895614624\n",
      "test: 0.8203125\n",
      "train: 0.8958333134651184\n",
      "test: 0.826171875\n",
      "train: 0.8928571343421936\n",
      "test: 0.8046875\n",
      "train: 0.8854166865348816\n",
      "test: 0.7900390625\n",
      "train: 0.8839285969734192\n",
      "test: 0.8544921875\n",
      "train: 0.8921130895614624\n",
      "test: 0.8779296875\n",
      "train: 0.8816964030265808\n",
      "test: 0.8427734375\n",
      "train: 0.8809523582458496\n",
      "test: 0.8330078125\n",
      "train: 0.8950892686843872\n",
      "test: 0.8720703125\n",
      "train: 0.898809552192688\n",
      "test: 0.8359375\n",
      "train: 0.8898809552192688\n",
      "test: 0.8427734375\n"
     ]
    }
   ],
   "source": [
    "# train eval loop\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "accs_train_outer = []\n",
    "accs_test_outer = []\n",
    "for e in range(200):\n",
    "    model.train()\n",
    "    accs_train = []\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, y = batch\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out[\"loss\"]\n",
    "        # train acc is here not calculated properly for simplification\n",
    "        accs_train.append(((torch.argmax(out[\"logits\"], dim=1) == y).sum() / batch_size).item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 20:\n",
    "            break\n",
    "    print(\"train:\", torch.tensor(accs_train).mean().item())\n",
    "    accs_train_outer.append(torch.tensor(accs_train).mean().item())\n",
    "    model.eval()\n",
    "    accs_test = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader_test):\n",
    "            x, y = batch\n",
    "            out = model(input_ids=x, labels=y)\n",
    "            accs_test.append(((torch.argmax(out[\"logits\"], dim=1) == y).sum() / batch_size).item())\n",
    "    print(\"test:\", torch.tensor(accs_test).mean().item())\n",
    "    accs_test_outer.append(torch.tensor(accs_test).mean().item())\n",
    "    torch.save(model, f\"model{e}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. use gradcam to deduct instance level prediction from bag level prediction \n",
    "- bag level: sentence contains typo y/n\n",
    "- instance level: word contains typo y/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(tensor: torch.Tensor, thresh: float) -> torch.Tensor:\n",
    "    return tensor > thresh\n",
    "\n",
    "def greater_zero_or_none(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    if tensor is not None:\n",
    "        return mask(tensor=tensor, thresh=0).int() * tensor\n",
    "    \n",
    "def rectify_hook(\n",
    "        module: torch.nn.Module,\n",
    "        grad_inputs: torch.Tensor,\n",
    "        grad_outputs: torch.Tensor):\n",
    "    new_grad_inputs = tuple([greater_zero_or_none(gi) for gi in grad_inputs])\n",
    "    return new_grad_inputs\n",
    "\n",
    "\n",
    "def display(text: str):\n",
    "    # load model and tokenizer and prepare for calculating gradcam on embeddings\n",
    "    model: BertForSequenceClassification = torch.load(\"model199.pt\")\n",
    "    for module in model.modules():\n",
    "        module.register_backward_hook(rectify_hook)\n",
    "    tok = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(tok(text)[\"input_ids\"]).unsqueeze(0)\n",
    "    e: torch.Tensor = model.bert.embeddings(input_ids)\n",
    "    e.requires_grad_(True)\n",
    "    e.retain_grad()\n",
    "\n",
    "    # calculate gradcam\n",
    "    logits = model.forward(inputs_embeds=e)[\"logits\"]\n",
    "    pred = np.round(torch.nn.functional.softmax(logits)[0][0].item(), 2)\n",
    "    logit = logits[0][0]\n",
    "    logit.backward()\n",
    "    grad = torch.mean(e.grad.data, dim=2).squeeze()\n",
    "    grad = grad[1:-1]\n",
    "    grad = grad + grad.min() * -1\n",
    "    grad = grad / grad.sum()\n",
    "\n",
    "    # merge tokens into words and sum up gradcam per word\n",
    "    tokens = tok.tokenize(text)\n",
    "    tokens_clean = [tokens[i] if tokens[i+1][:2] == \"##\" else tokens[i] + \" \" for i in range(len(tokens) - 1)]\n",
    "    tokens_clean.append(tokens[-1])\n",
    "    tokens_clean = [t[2:] if t[:2] == \"##\" else t for t in tokens_clean]\n",
    "    word_grads = []\n",
    "    words = []\n",
    "    grad_val = 0\n",
    "    word = \"\"\n",
    "    for i in range(len(tokens_clean)):\n",
    "        if tokens_clean[i][-1] == \" \":\n",
    "            word += tokens_clean[i]\n",
    "            grad_val += grad[i]\n",
    "            word_grads.append(grad_val)\n",
    "            words.append(word)\n",
    "            word = \"\"\n",
    "            grad_val = 0\n",
    "        else:\n",
    "            word += tokens_clean[i]\n",
    "            grad_val += grad[i]\n",
    "            if i == (len(tokens_clean) - 1):\n",
    "                words.append(word)\n",
    "                word_grads.append(grad_val)\n",
    "    words = [w[:-1] if w[-1] == \" \" else w for w in words]\n",
    "    word_grads = [g.item() for g in word_grads]\n",
    "    word_grads = np.array(word_grads).reshape(1, -1)\n",
    "\n",
    "    # plot results\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    heatmap = ax.imshow(word_grads.reshape(1, -1), cmap='Reds', aspect='auto')\n",
    "\n",
    "    # Adding labels within each cell\n",
    "    for i, label in enumerate(words):\n",
    "        ax.text(i, 0, label, ha='center', va='center', color='black')\n",
    "\n",
    "    ax.set_title(f\"word-level gradcam\\nprediction: sentence contains typo (confidence: {str(pred)})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"gradcam.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\anaconda3\\envs\\small_machine_learning_projects_light\\lib\\site-packages\\torch\\nn\\modules\\module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "c:\\Users\\milan\\anaconda3\\envs\\small_machine_learning_projects_light\\lib\\site-packages\\torch\\nn\\modules\\module.py:1319: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "c:\\Users\\milan\\anaconda3\\envs\\small_machine_learning_projects_light\\lib\\site-packages\\torch\\nn\\modules\\module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_160\\3149660177.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = np.round(torch.nn.functional.softmax(logits)[0][0].item(), 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Die Zwiebeln udn das Ei in eine Schüssel geben.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_machine_learning_projects_light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

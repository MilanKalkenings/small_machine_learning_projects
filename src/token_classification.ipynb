{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\OneDrive\\Documents\\portfolio\\llama_index_projects\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pip install llama-index\n",
    "pip install llama-index-llms-huggingface\n",
    "pip install \"transformers[torch]\" \"huggingface_hub\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from random import random\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI  # 0.10.25\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForTokenClassification\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data with Semi-Supervised Labels from an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to LLM\n",
    "llm_checkpoint = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "remotely_run = HuggingFaceInferenceAPI(model_name=llm_checkpoint, token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-generated text blocks\n",
    "nachnamen = list(set([\n",
    "    \"Müller\", \"Schmidt\", \"Schneider\", \"Fischer\", \"Weber Meier\", \"Meyer\", \"Wagner\", \"Becker\",\n",
    "    \"Schulz\", \"Berg Hoffmann\", \"Schäfer\", \"Koch\", \"Bauer\", \"Richter\", \"Klein\", \"Wolf\", \n",
    "    \"Schröder\", \"Neumann-Schmid\", \"Schwarz\", \"Zimmermann\", \"Braun\", \"Krüger\", \"Hofmann\", \"Hartmann\", \n",
    "    \"Lange\", \"Schmitt\", \"Werner\", \"Schmitz\", \"Krause\", \"Meier\", \"Lehmann\", \"Schmid\", \"Schulze\", \n",
    "    \"Maier\", \"Köhler\", \"Herrmann\", \"Öztürk\", \"König\", \"Walter\", \"Mayer\", \"Huber\", \"Kaiser\", \"Fuchs\", \n",
    "    \"Peters\", \"Lang-Maler\", \"Scholz\", \"Möller\", \"Weiß\", \"Jung\", \"Hahn\", \"Schubert\", \"Vogel\", \"Friedrich\", \n",
    "    \"Keller\", \"Günther\", \"Frank\", \"Berger\", \"Winkler\", \"Roth\", \"Beck\", \"Lorenz Fischer\", \"Baumann\", \n",
    "    \"Franke\", \"Albrecht\", \"Schuster\", \"Simon\", \"Ludwig\", \"Böhm\", \"Winter\", \"Kraus\", \"Martin\", \n",
    "    \"Schumacher\", \"Krämer\", \"Vogt\", \"Stein\", \"Jäger\", \"Otto\", \"Sommer\", \"Groß\", \"Seidel\", \n",
    "    \"Heinrich\", \"Brandt\", \"Haas\", \"Schreiber\", \"Graf\", \"Schlegel\", \"Dietrich\", \"Ziegler\", \"Kuhn\", \n",
    "    \"Kühn\", \"Pohl\", \"Engel\", \"Horn\", \"Busch\", \"Bergmann\", \"Thomas\", \"Voigt\", \"Sauer\", \"Arnold\", \n",
    "    \"Wolff\", \"Pfeiffer\", \"Wolf-Richter\", \"Yılmaz\", \"Kaya\", \"Demir Schmidt\", \"Şahin\", \"Çelik\", \"Yıldız\", \"Yıldırım\", \"Öztürk\", \"Aydın\", \"Özdemir\",\n",
    "    \"Arslan\", \"Doğan\", \"Kılıç\", \"Aslan\", \"Çetin\", \"Karadağ\", \"Koç\", \"Kurt\", \"Özkan\", \"Akar\",\n",
    "    \"Acar\", \"Tekin\", \"Kara\", \"Ekici\", \"Kaplan\", \"Şimşek\", \"Avcı\", \"Güler\", \"Korkmaz\", \"Sarı\",\n",
    "    \"Balcı\", \"Selvi\", \"Göçer\", \"Polat\", \"Demirci\", \"Duman\", \"Tuna\", \"Taş\", \"Keskin\", \"Güneş\",\n",
    "    \"Can\", \"Aydemir\", \"Ata\", \"Özer-Maler\", \"Çiftçi\", \"Bayraktar\", \"Erdoğan\", \"Bozkurt\", \"Kan\", \"Dağ\", \n",
    "    \"Nowak\", \"Kowalski\", \"Wiśniewski\", \"Dąbrowski\", \"Lewandowski\", \"Wójcik\", \"Kamiński\", \"Kowalczyk\", \"Zieliński\", \"Szymański\",\n",
    "    \"Woźniak\", \"Kozłowski\", \"Jankowski\", \"Wojciechowski\", \"Kwiatkowski\", \"Kaczmarek\", \"Mazur\", \"Kubiak\", \"Król\", \"Pawłowski\"]))\n",
    "\n",
    "vornamen = list(set([\"Leon\", \"Mustafa\", \"Lukas\", \"Finn\", \"Noah\", \"Paul\", \"Jonas\", \"Luis\", \"Elias\", \"Felix\", \"Luca\",\n",
    "    \"Max\", \"Henry\", \"Julian\", \"Niklas\", \"Tim\", \"Alexander\", \"Philipp\", \"David\", \"Maximilian\", \"Liam\",\n",
    "    \"Oskar\", \"Moritz\", \"Fabian\", \"Simon\", \"Erik\", \"Jakob-Mark\", \"Vincent\", \"Benjamin\", \"Matteo\", \"Anton\",\n",
    "    \"Emil\", \"Carl\", \"Jonathan\", \"Theo\", \"Samuel\", \"Linus\", \"Mats\", \"Jan\", \"Nico\", \"Leonard\",\n",
    "    \"Hannes\", \"Florian\", \"Ben\", \"Adam\", \"Raphael\", \"Tobias\", \"Sebastian\", \"Martin\", \"Johannes\", \"Fabio\",\n",
    "    \"Lennard\", \"Michael\", \"Jona\", \"Joshua\", \"Marcel\", \"Tom\", \"Valentin\", \"Lennart\", \"Levin\", \"Maxim\",\n",
    "    \"Kilian\", \"Konstantin\", \"Robin\", \"Lars\", \"Emilian\", \"Arne\", \"Matthias\", \"Milan\", \"Mohammed\", \"Kai\",\n",
    "    \"Nick\", \"Ole Joost\", \"Julius\", \"Benedikt\", \"Marvin\", \"Leopold\", \"Nils\", \"Daniel\", \"Franz\", \"Manuel\",\n",
    "    \"Noel\", \"Pascal\", \"Mika\", \"Adrian\", \"Oliver\", \"Stefan\", \"Lorenz\", \"Valentino\", \"Magnus\", \"Jan-Phillip\",\n",
    "    \"Constantin\", \"Artur\", \"Albert\", \"Frederik\", \"Hugo\", \"Timo\", \"Jasper\", \"Aron\", \"Joel\", \"Christian\",\n",
    "    \"Anna-Lena\", \"Maria\", \"Emma\", \"Sofia\", \"Mia\", \"Hannah\", \"Lena\", \"Sarah\", \"Lea\", \"Laura\",\n",
    "    \"Katharina\", \"Lisa\", \"Julia\", \"Sophie\", \"Isabella\", \"Charlotte\", \"Lara\", \"Marie\", \"Clara\", \"Lina\",\n",
    "    \"Luisa\", \"Johanna\", \"Paula\", \"Emilia\", \"Antonia\", \"Theresa\", \"Luise\", \"Helena\", \"Elisabeth\", \"Nina\",\n",
    "    \"Magdalena\", \"Melanie\", \"Anja\", \"Christina\", \"Sandra\", \"Annika\", \"Silke\", \"Katja\", \"Veronika\", \"Monika\",\n",
    "    \"Birgit\", \"Sabine\", \"Petra\", \"Jana Susanne\", \"Simone\", \"Annette\", \"Stefanie\", \"Nicole\", \"Barbara\", \"Sonja\",\n",
    "    \"Carina\", \"Yvonne\", \"Daniela\", \"Eva\", \"Heike\", \"Tanja\", \"Ingrid\", \"Franziska\", \"Renate\", \"Irina\",\n",
    "    \"Gisela\", \"Martina\", \"Andrea\", \"Ursula\", \"Ines\", \"Beate\", \"Gabriele\", \"Cornelia\", \"Diana\", \"Brigitte\",\n",
    "    \"Elena\", \"Valentina\", \"Alicia\", \"Maja\", \"Anastasia\", \"Karina\", \"Doris\", \"Judith\", \"Frieda\", \"Irma\",\n",
    "    \"Hilde\", \"Erika\", \"Margarete\", \"Elfriede\", \"Gertrud\", \"Edith\", \"Ruth\", \"Ilse\", \"Hedwig\", \"Lieselotte\",\n",
    "    \"Klara\", \"Olga\", \"Rita\", \"Waltraud\", \"Inge\", \"Herta\", \"Martha\", \"Else\", \"Ute\", \"Helga Marie\", \"Ahmet\", \n",
    "    \"Mehmet\", \"Mustafa\", \"Ayşe\", \"Fatma Binnaz\", \"Yusuf\", \"Zeynep\",\n",
    "    \"Elif\", \"Ömer\", \"Emir\", \"Hüseyin\", \"Hasan\", \"Ali\", \"Ibrahim\",\n",
    "    \"Sümeyye\", \"Hatice\", \"Ece\", \"Kerem\", \"Büşra\", \"Taha\", \"Rümeysa\",\n",
    "    \"Furkan\", \"Selin\", \"Cem\", \"Esra\", \"Berk\", \"Derya\", \"Merve\", \"Cansu\", \"Deniz\", \"Aleksandra\", \"Aneta\", \"Bartosz\", \n",
    "    \"Czesław\", \"Daria\",\"Emilia\", \"Filip\", \"Grażyna\", \"Henryk\", \"Iwona\",\n",
    "    \"Jakub\", \"Katarzyna\", \"Łukasz\", \"Małgorzata\", \"Natalia\",\n",
    "    \"Oskar\", \"Paweł\", \"Róża\", \"Szymon\", \"Tomasz\"]))\n",
    "\n",
    "roles = [\"Verkehrspolizist\", \"Rechtsanwalt\", \"Beschuldigter\", \"Beschädigter\", \"Zeuge\", \"Reporter\"]\n",
    "\n",
    "categories = [\n",
    "    \"Überhöhte Geschwindigkeit\",\n",
    "    \"Alkohol am Steuer\",\n",
    "    \"Unaufmerksamkeit des Fahrers\",\n",
    "    \"Nichtbeachtung von Verkehrszeichen\",\n",
    "    \"Unangepasste Geschwindigkeit bei schlechtem Wetter\",\n",
    "    \"Fahren unter Drogeneinfluss\",\n",
    "    \"Ablenkung durch Handynutzung\",\n",
    "    \"Ermüdung des Fahrers\",\n",
    "    \"Technisches Versagen des Fahrzeugs\",\n",
    "    \"Fehler beim Abbiegen\",\n",
    "    \"Missachtung der Vorfahrt\",\n",
    "    \"Fehlerhaftes Überholen\",\n",
    "    \"Mangelnder Sicherheitsabstand\",\n",
    "    \"Falschfahren auf der Autobahn\",\n",
    "    \"Schlechte Straßenverhältnisse\",\n",
    "    \"Nichtanpassung der Geschwindigkeit an Verkehrsdichte\",\n",
    "    \"Fahren ohne gültige Fahrerlaubnis\",\n",
    "    \"Unsicheres Wechseln der Fahrstreifen\",\n",
    "    \"Defekte Verkehrssignale\",\n",
    "    \"Unzureichende Fahrzeugbeleuchtung\",\n",
    "    \"Fehler beim Rückwärtsfahren\",\n",
    "    \"Ladungssicherungsmängel\",\n",
    "    \"Fehlverhalten von Fußgängern\",\n",
    "    \"Aggressives Fahren\",\n",
    "    \"Missachtung der Fußgängerüberwege\",\n",
    "    \"Unfälle durch Wildwechsel\",\n",
    "    \"Unzureichende Kennzeichnung von Baustellen\",\n",
    "    \"Mängel an der Bremsanlage\",\n",
    "    \"Reifenpannen\",\n",
    "    \"Fahren mit überladenen Fahrzeugen\",\n",
    "    \"Nichtbeachtung von Stoppschildern\",\n",
    "    \"Fehler beim Einfahren in einen Kreisverkehr\",\n",
    "    \"Unaufmerksamkeit beim Ausparken\",\n",
    "    \"Verwendung von Alkohol oder Drogen durch Fußgänger\",\n",
    "    \"Kollisionen beim Spurwechsel\",\n",
    "    \"Unfälle in Kreuzungsgebieten\",\n",
    "    \"Zu dichtes Auffahren\",\n",
    "    \"Unerlaubtes Wenden auf Straßen\",\n",
    "    \"Schlechte Sichtverhältnisse\",\n",
    "    \"Fehlerhafte Verkehrsplanung\",\n",
    "    \"Nicht beachten von Ampelsignalen\",\n",
    "    \"Verkehrsunfälle durch Tierkollision\",\n",
    "    \"Fahren auf der falschen Fahrbahnseite\",\n",
    "    \"Unfälle verursacht durch gesundheitliche Probleme\",\n",
    "    \"Verkehrsunsicherer Zustand des Fahrzeugs\",\n",
    "    \"Starkes Beschleunigen\",\n",
    "    \"Unfälle in Baustellenbereichen\",\n",
    "    \"Fahren ohne Sicherheitsgurt\",\n",
    "    \"Unfälle durch Fahrerflucht\",\n",
    "    \"Irrtum des Fahrers bezüglich der Verkehrsregeln\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Julius Bayraktar\n",
      "\n",
      "\n",
      "<satz>Julius Bayraktar bemerkte die Verkehrsdichte zu spät und fuhr mit seinem Fahrzeug ungebremst auf das vor ihm stehende Auto auf.</satz>\n",
      "1 Nico Şahin\n",
      "\n",
      "\n",
      "<satz>Nico Şahin, der unaufmerksam war, verursachte einen Kfz-Unfall.</satz>\n",
      "rate limit reached\n"
     ]
    }
   ],
   "source": [
    "# use generated text blocks to create versatile generated texts with semi supervised labels & save texts on disk\n",
    "for i in range(50):\n",
    "    try:\n",
    "        # select role, category and name\n",
    "        role = roles[int(random() * len(roles))]\n",
    "        category = categories[int(random() * len(categories))]\n",
    "        vorname = vornamen[int(random() * len(vornamen))]\n",
    "        nachname = nachnamen[int(random() * len(nachnamen))]\n",
    "        # name variations: Herr/Frau <last_name> or <forename> <last_name> or <forename>\n",
    "        rand = random()\n",
    "        if rand < 0.1:\n",
    "            name = \"Herr \" + nachname\n",
    "        elif rand < 0.2:\n",
    "            name = \"Frau \" + nachname\n",
    "        elif rand < 0.3:\n",
    "            name = vorname\n",
    "        else:\n",
    "            name = vorname + \" \" + nachname\n",
    "        if os.path.exists(f\"generated_texts/company {name}.txt\"):\n",
    "            continue\n",
    "        print(i, name)\n",
    "        # use LLM to generate text\n",
    "        prompt = f\"Du bist {role}. Schreibe einen Satz, welcher einen Kfz-Unfall der Kategorie {category} beschreibt und den Namen '''{name}''' enthält! Gehe nicht auf zeitliche Details ein. Beginne den Satz mit dem Unfallhergang! Schreibe den Satz in <satz><\\satz> XML-Tags!\"\n",
    "        text = remotely_run.complete(prompt).text\n",
    "        print(text)\n",
    "        # save text\n",
    "        with open(f\"generated_texts/{name}.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "    except:\n",
    "        print(\"rate limit reached\")\n",
    "        break  # time.sleep(10 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Process LLM-Generated Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts: List[str], to_label: List[str], label_as: List[int], tokenizer: BertTokenizer) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    creates the tensor data set, padding to maximum length.\n",
    "    out: token_ids, token_labels, attention_mask\n",
    "    \"\"\"\n",
    "    max_len = 0\n",
    "    text_ids_dataset = []\n",
    "    labels_dataset = []\n",
    "    attention_mask_dataset = []\n",
    "    # tokenize each text and assign token labels\n",
    "    for i in range(len(texts)):\n",
    "        text_ids, labels = ids_labels(text=texts[i], to_label=to_label[i], label=label_as[i], tokenizer=tokenizer)\n",
    "        text_ids_dataset.append(text_ids)\n",
    "        labels_dataset.append(labels)\n",
    "        attention_mask_dataset.append([1] * len(labels))\n",
    "        if len(labels) > max_len:\n",
    "            max_len = len(labels)\n",
    "    # pad according to longest text\n",
    "    for i in range(len(texts)):\n",
    "        if len(text_ids_dataset[i]) < max_len:\n",
    "            text_ids_dataset[i] = text_ids_dataset[i] + [tokenizer.pad_token_id] * (max_len - len(text_ids_dataset[i]))\n",
    "            labels_dataset[i] = labels_dataset[i] + [0] * (max_len - len(labels_dataset[i]))\n",
    "            attention_mask_dataset[i] = attention_mask_dataset[i] + [0] * (max_len - len(attention_mask_dataset[i]))\n",
    "    text_ids_dataset = torch.tensor(text_ids_dataset)\n",
    "    labels_dataset = torch.tensor(labels_dataset)\n",
    "    attention_mask_dataset = torch.tensor(attention_mask_dataset)\n",
    "    return text_ids_dataset, labels_dataset, attention_mask_dataset\n",
    "\n",
    "def ids_labels(text: str, to_label: str, label: int, tokenizer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    finds <to_label> in <text> and labels respective tokens with <label>, all other with 0.\n",
    "    TODO: allow labeling multiple sequences per text; allow labeling by position spans, not by to_label-string\n",
    "    \"\"\"\n",
    "    # fix labels according to grammar rules\n",
    "    if (\" \" + to_label + \"s\" + \" \") in text:\n",
    "        to_label = to_label + \"s\"\n",
    "    elif \"Herr \" in to_label:\n",
    "        if to_label in text:\n",
    "            pass\n",
    "        else:\n",
    "            if \"Herrn \" + to_label[5:] in text:\n",
    "                to_label = \"Herrn \" + to_label[5:]\n",
    "    tokens_label = tokenizer.tokenize(to_label)\n",
    "    tokens_text = tokenizer.tokenize(text)\n",
    "    labels = []\n",
    "    last_found_pos = - (len(tokens_text) + 1)  # initiate with impossible value\n",
    "    for i in range(len(tokens_text)):\n",
    "        if i < (last_found_pos + len(tokens_label)):\n",
    "            continue\n",
    "        matches = True\n",
    "        for j in range(len(tokens_label)):\n",
    "            if tokens_label[j] != tokens_text[i + j]:\n",
    "                matches = False\n",
    "                break\n",
    "        if matches:\n",
    "            labels.extend([label] * len(tokens_label))\n",
    "            last_found_pos = i\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return tokenizer(text)[\"input_ids\"], [0] + labels + [0]\n",
    "\n",
    "\n",
    "def discard_zero_rows(token_ids: torch.Tensor, token_labels: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    discard rows where all tokens are labeled as 0.\n",
    "    Note: it might be beneficial to keep few all 0 sequences in training data to demonstrate, \n",
    "    that some sentences don't contain any label\n",
    "    \"\"\"\n",
    "    mask = torch.max(token_labels, dim=1).values > 0\n",
    "    return token_ids[mask], token_labels[mask], attention_mask[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([860, 76])\n",
      "torch.Size([847, 76])\n"
     ]
    }
   ],
   "source": [
    "# discard faulty texts\n",
    "max_len = 250  # too long text -> sth likely went wrong\n",
    "texts = []\n",
    "label_as = []\n",
    "to_label = []\n",
    "for file in os.listdir(\"generated_texts\"):\n",
    "    with open(f\"generated_texts/{file}\", \"r\") as f:\n",
    "        generated = f.read()\n",
    "        generated_between_tags = re.findall(pattern=r\"(?<=<satz>).+(?=</satz>|<\\\\satz>)\", string=generated, flags=re.DOTALL)\n",
    "        if len(generated_between_tags) == 1:  # otherwise something went wrong in generation\n",
    "            text = generated_between_tags[0]\n",
    "            text = re.sub(pattern=\"'\", repl=\"\", string=text)  # often indicate names in the generated texts\n",
    "            text = re.sub(pattern=\"\\s+\", repl=\" \", string=text)  # multiple whitespaces\n",
    "            text = text.strip()  # remove leading and trailing whitespaces\n",
    "            if len(text) <= max_len:  # discard too long texts\n",
    "                texts.append(text)\n",
    "                to_label.append(file[:-4])\n",
    "                label_as.append(1)\n",
    "# create tensor dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "token_ids, token_labels, attention_mask = create_dataset(texts=texts, label_as=label_as, to_label=to_label, tokenizer=tokenizer)\n",
    "print(token_labels.shape)\n",
    "token_ids, token_labels, attention_mask = discard_zero_rows(token_ids=token_ids, token_labels=token_labels, attention_mask=attention_mask)  # discard further errors\n",
    "print(token_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Outlier Sampling for Train-Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_rank(vector: torch.Tensor, vector_set: torch.Tensor):\n",
    "    distances = torch.norm(vector - vector_set, dim=1)\n",
    "    return  torch.argsort(distances, descending=True)\n",
    "\n",
    "def distance_rank_fused(vectors, vector_set: torch.Tensor):\n",
    "    ranks = [distance_rank(v, vector_set) for v in vectors]\n",
    "    ranking_fused = []\n",
    "    for i in range(len(vector_set)):\n",
    "        fused_rank = 0\n",
    "        for rank in ranks:        \n",
    "            rank: torch.Tensor = rank\n",
    "            fused_rank += torch.where(rank == i)[0]\n",
    "        ranking_fused.append(float(fused_rank / len(ranks)))\n",
    "    return torch.tensor(ranking_fused)\n",
    "\n",
    "def edgecase_sampling(embeddings: torch.Tensor, n: int) -> List[int]:\n",
    "    mean = torch.mean(embeddings, dim=0)\n",
    "    sample = [mean]\n",
    "    sample_ids = []\n",
    "    # iteratively select data point that is on average most distant from all others\n",
    "    for i in range(n):\n",
    "        dist_rank = distance_rank_fused(vectors=sample, vector_set=embeddings)\n",
    "        id_most_distant = torch.argmin(dist_rank)\n",
    "        sample.append(embeddings[id_most_distant])\n",
    "        embeddings = torch.cat((embeddings[:id_most_distant], embeddings[(id_most_distant + 1):]))\n",
    "        sample_ids.append(id_most_distant)\n",
    "    return sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_16464\\2924086422.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices_eval = edgecase_sampling(embeddings=torch.tensor(embeddings), n=eval_size)\n"
     ]
    }
   ],
   "source": [
    "eval_size = 64\n",
    "\n",
    "# create bert embeddings for all data points\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in torch.arange(start=0, end=len(token_ids), step=32):\n",
    "        embeddings.append(torch.mean(model.bert(input_ids=token_ids[i:i+32], attention_mask=attention_mask[i:i+32]).last_hidden_state, dim=1))\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "# robust outlier sampling\n",
    "indices_eval = edgecase_sampling(embeddings=torch.tensor(embeddings), n=eval_size)\n",
    "\n",
    "# split:\n",
    "# eval data\n",
    "token_ids_eval = token_ids[indices_eval]\n",
    "token_labels_eval = token_labels[indices_eval]\n",
    "attention_mask_eval = attention_mask[indices_eval]\n",
    "# train data\n",
    "token_ids_train = token_ids[[i for i in range(len(token_ids)) if i not in indices_eval]]\n",
    "token_labels_train = token_labels[[i for i in range(len(token_ids)) if i not in indices_eval]]\n",
    "attention_mask_train = attention_mask[[i for i in range(len(token_ids)) if i not in indices_eval]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred: torch.Tensor, true: torch.Tensor) -> float:\n",
    "    return float((pred == true).sum() / len(true.flatten()))\n",
    "\n",
    "def recall(pred: torch.Tensor, true: torch.Tensor, on: int = 1) -> float:\n",
    "    pred = pred.flatten()\n",
    "    true = true.flatten()\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i] == on:\n",
    "            correct += 1\n",
    "    return correct / len(true[true == on])\n",
    "\n",
    "def random_batch(token_ids: torch.Tensor, token_labels: torch.Tensor, attention_mask: torch.Tensor, batch_size: int):\n",
    "    indices = torch.randint(low=0, high=len(token_ids), size=(batch_size,))\n",
    "    return token_ids[indices], token_labels[indices], attention_mask[indices], indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00005)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accuracy eval: 0.9636101722717285 recall eval: 0.0 \n",
      "\n",
      "1 accuracy eval: 0.9636101722717285 recall eval: 0.0 \n",
      "\n",
      "2 accuracy eval: 0.9636101722717285 recall eval: 0.0 \n",
      "\n",
      "3 accuracy eval: 0.9636101722717285 recall eval: 0.0 \n",
      "\n",
      "4 accuracy eval: 0.9636101722717285 recall eval: 0.0 \n",
      "\n",
      "5 accuracy eval: 0.9732730388641357 recall eval: 0.2655367231638418 \n",
      "\n",
      "6 accuracy eval: 0.9917762875556946 recall eval: 0.7740112994350282 \n",
      "\n",
      "7 accuracy eval: 0.9991776347160339 recall eval: 0.9774011299435028 \n",
      "\n",
      "8 accuracy eval: 0.9989720582962036 recall eval: 0.9830508474576272 \n",
      "\n",
      "9 accuracy eval: 0.9983552694320679 recall eval: 0.9943502824858758 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train-eval loop\n",
    "for i in range(20):\n",
    "    # train\n",
    "    model.train()\n",
    "    token_ids_batch, token_labels_batch, attention_mask_batch, _ = random_batch(\n",
    "        token_ids=token_ids_train, \n",
    "        token_labels=token_labels_train,\n",
    "        attention_mask=attention_mask_train,\n",
    "        batch_size=batch_size)\n",
    "    out = model.forward(input_ids=token_ids_batch, labels=token_labels_batch, attention_mask=attention_mask_batch)\n",
    "    loss: torch.Tensor = out[\"loss\"]\n",
    "    logits: torch.Tensor = out[\"logits\"]\n",
    "    pred = torch.argmax(logits, dim=2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # eval\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # on evaluation data\n",
    "        out = model.forward(input_ids=token_ids_eval, labels=token_labels_eval, attention_mask=attention_mask_eval)\n",
    "        logits: torch.Tensor = out[\"logits\"]\n",
    "        pred = torch.argmax(logits, dim=2)\n",
    "        acc_eval = acc(pred=pred, true=token_labels_eval)\n",
    "        rec_eval = recall(true=token_labels_eval, pred=pred) \n",
    "        print(i, \"accuracy eval:\", acc_eval, \"recall eval:\", rec_eval, \"\\n\")\n",
    "        if rec_eval > 0.99:\n",
    "            break\n",
    "# save to disk\n",
    "torch.save(model.state_dict(), \"model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on OOD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Henri', '##ette', 'Emil', '##io']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "text = \"Henriette arbeitet heute zusammen mit Krankenschwestern. In der Uni hat Emilio eine neue Methode gelernt.\"\n",
    "with torch.no_grad():\n",
    "    ids, labels, mask = create_dataset(texts=[text], to_label=[\"Henriette\"], label_as=[1], tokenizer=tokenizer)\n",
    "    pred = torch.argmax(model(input_ids=ids, attention_mask=mask)[\"logits\"], dim=2)\n",
    "    tokens = [0] + tokenizer.tokenize(text)\n",
    "    print([t for i, t in enumerate(tokens) if pred[0][i] == 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
